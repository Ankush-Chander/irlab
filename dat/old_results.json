[
    {
        "date": "2011-01-01",
        "raw_text": "We present Biological Sequence Matching Using Fuzzy Logic. Sequence alignment is the most basic and essential module of computational bio-informatics. In this paper, we propose a multiple sequence alignment algorithm that employs fuzzy logic to measure the similarity of sequences based on fuzzy parameters. To guarantee the optimal alignment of the sequences, dynamic programming is used to align the sequences. The algorithm is tested on few sets of real biological sequences taken from NCBI bank and its performance is evaluated using SinicView tool.",
        "title": "Biological Sequence Matching Using Fuzzy Logic",
        "doc_id": "5bd22f701aba23eed8ceabb17a582299",
        "_score": 124.52514,
        "objective_sentences": "We present Biological Sequence Matching Using Fuzzy Logic. In this paper, we propose a multiple sequence alignment algorithm that employs fuzzy logic to measure the similarity of sequences based on fuzzy parameters.",
        "sem_score": 4.4
    },
    {
        "date": "2018-01-01",
        "raw_text": "We present Word Embedding based Semantic Cross-Lingual Document Alignment in Comparable Corpora. Crosslingual information retrieval (CLIR) finds its application in aligning documents across comparable corpora. However, traditional CLIR, due to the term independence assumption, cannot consider the semantic similarity between the constituent words of the candidate pairs of documents in two different languages. Moreover, traditional CLIR models score a document by aggregating only the weights of the constituent terms that match with those of the query, while the other non-matching terms of the document do not significantly contribute to the similarity function. Word vector embedding allows the provision to model the semantic distances between terms by the application of standard distance metrics between their corresponding real valued vectors. This paper develops a word vector embedding based CLIR model that uses the average distances between the embedded word vectors of the source and target language documents to rank candidate document pairs. Our experiments with the WMT bilingual document alignment dataset reveal that the word vector based similarity significantly improves the recall of crosslingual document alignment in comparison to the classical language modeling based CLIR.",
        "title": "Word Embedding based Semantic Cross-Lingual Document Alignment in Comparable Corpora",
        "doc_id": "30b575c94418f41d3b070a33acca8d4d",
        "_score": 177.21043,
        "objective_sentences": "We present Word Embedding based Semantic Cross-Lingual Document Alignment in Comparable Corpora. This paper develops a word vector embedding based CLIR model that uses the average distances between the embedded word vectors of the source and target language documents to rank candidate document pairs.",
        "sem_score": 3.4523809523809526
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present Word Embedding Models for Finding Semantic Relationship between Words in Tamil Language. Objective: Word embedding models were most predominantly used in many of the NLP tasks such as document classification, author identification, story understanding etc. In this paper we make a comparison of two Word embedding models for semantic similarity in Tamil language. Each of those two models has its own way of predicting relationship between words in a corpus. Method/Analysis: The term Word embedding in Natural Language Processing is a representation of words in terms of vectors. Word embedding is used as an unsupervised approach instead of traditional way of feature extraction. Word embedding models uses neural networks to generate numerical representation for the given words. In order to find the best model that captures semantic relationship between words, using a morphologically rich language like Tamil would be great. Tamil language is one of the oldest Dravidian languages and it is known for its morphological richness. In Tamil language it is possible to construct 10,000 words from a single root word. Findings: Here we make comparison of Content based Word embedding and Context based Word embedding models respectively. We tried different feature vector sizes for the same word to comment on the accuracy of the models for semantic similarity. Novelty/Improvement: Analysing Word embedding models for morphologically rich language like Tamil helps us to classify the words better based on its semantics.",
        "title": "Word Embedding Models for Finding Semantic Relationship between Words in Tamil Language",
        "doc_id": "605ad90c88d7a07b9274e773a8e7a096",
        "_score": 117.79023,
        "objective_sentences": "Objective: Word embedding models were most predominantly used in many of the NLP tasks such as document classification, author identification, story understanding etc. In this paper we make a comparison of two Word embedding models for semantic similarity in Tamil language.",
        "sem_score": 3.375163398692811
    },
    {
        "raw_text": "We present Similarity of Sentences With Contradiction Using Semantic Similarity Measures. AbstractShort text or sentence similarity is crucial in various natural language processing activities. Traditional measures for sentence similarity consider word order, semantic features and role annotations of text to derive the similarity. These measures do not suit short texts or sentences with negation. Hence, this paper proposes an approach to determine the semantic similarity of sentences and also presents an algorithm to handle negation. In sentence similarity, word pair similarity plays a significant role. Hence, this paper also discusses the similarity between word pairs. Existing semantic similarity measures do not handle antonyms accurately. Hence, this paper proposes an algorithm to handle antonyms. This paper also presents an antonym dataset with 111-word pairs and corresponding expert ratings. The existing semantic similarity measures are tested on the dataset. The results of the correlation proved that the expert ratings are in order with the correlation obtained from the semantic similarity measures. The sentence similarity is handled by proposing two algorithms. The first algorithm deals with the typical sentences, and the second algorithm deals with contradiction in the sentences. SICK dataset, which has sentences with negation, is considered for handling the sentence similarity. The algorithm helped in improving the results of sentence similarity.",
        "date": "2020-08-19",
        "title": "Similarity of Sentences With Contradiction Using Semantic Similarity Measures",
        "doc_id": "5aaf8c8286d9b986b03b553ea38d1009",
        "_score": 114.31652,
        "objective_sentences": "We present Similarity of Sentences With Contradiction Using Semantic Similarity Measures. Hence, this paper proposes an approach to determine the semantic similarity of sentences and also presents an algorithm to handle negation.",
        "sem_score": 3.3333333333333335
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present Prediction of G protein-coupled receptor encoding sequences from the synganglion transcriptome of the cattle tick, Rhipicephalus microplus. The cattle tick, Rhipicephalus (Boophilus) microplus, is a pest which causes multiple health complications in cattle. The G protein-coupled receptor (GPCR) super-family presents a candidate target for developing novel tick control methods. However, GPCRs share limited sequence similarity among orthologous family members, and there is no reference genome available for R. microplus. This limits the effectiveness of alignment-dependent methods such as BLAST and Pfam for identifying GPCRs from R. microplus. However, GPCRs share a common structure consisting of seven transmembrane helices. We present an analysis of the R. microplus synganglion transcriptome using a combination of structurally-based and alignment-free methods which supplement the identification of GPCRs by sequence similarity. TMHMM predicts the number of transmembrane helices in a protein sequence. GPCRpred is a support vector machine-based method developed to predict and classify GPCRs using the dipeptide composition of a query amino acid sequence. These two bioinformatic tools were applied to our transcriptome assembly of the cattle tick synganglion. Together, BLAST and Pfam identified 85 unique contigs as encoding partial or full length candidate cattle tick GPCRs. Collectively, TMHMM and GPCRpred identified 27 additional GPCR candidates that BLAST and Pfam missed. This demonstrates that the addition of structurally-based and alignment-free bioinformatic approaches to transcriptome annotation and analysis produces a greater collection of prospective GPCRs than an analysis based solely upon methodologies dependent upon sequence alignment and similarity.",
        "title": "Prediction of G protein-coupled receptor encoding sequences from the synganglion transcriptome of the cattle tick, Rhipicephalus microplus",
        "doc_id": "207840ed70e56993da252f8108c0595e",
        "_score": 129.3796,
        "objective_sentences": "We present Prediction of G protein-coupled receptor encoding sequences from the synganglion transcriptome of the cattle tick, Rhipicephalus microplus. We present an analysis of the R. microplus synganglion transcriptome using a combination of structurally-based and alignment-free methods which supplement the identification of GPCRs by sequence similarity.",
        "sem_score": 3.25
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present Prediction of G protein-coupled receptor encoding sequences from the synganglion transcriptome of the cattle tick, Rhipicephalus microplus. The cattle tick, Rhipicephalus (Boophilus) microplus, is a pest which causes multiple health complications in cattle. The G protein-coupled receptor (GPCR) super-family presents a candidate target for developing novel tick control methods. However, GPCRs share limited sequence similarity among orthologous family members, and there is no reference genome available for R. microplus. This limits the effectiveness of alignment-dependent methods such as BLAST and Pfam for identifying GPCRs from R. microplus. However, GPCRs share a common structure consisting of seven transmembrane helices. We present an analysis of the R. microplus synganglion transcriptome using a combination of structurally-based and alignment-free methods which supplement the identification of GPCRs by sequence similarity. TMHMM predicts the number of transmembrane helices in a protein sequence. GPCRpred is a support vector machine-based method developed to predict and classify GPCRs using the dipeptide composition of a query amino acid sequence. These two bioinformatic tools were applied to our transcriptome assembly of the cattle tick synganglion. Together, BLAST and Pfam identified 85 unique contigs as encoding partial or full length candidate cattle tick GPCRs. Collectively, TMHMM and GPCRpred identified 27 additional GPCR candidates that BLAST and Pfam missed. This demonstrates that the addition of structurally-based and alignment-free bioinformatic approaches to transcriptome annotation and analysis produces a greater collection of prospective GPCRs than an analysis based solely upon methodologies dependent upon sequence alignment and similarity.",
        "title": "Prediction of G protein-coupled receptor encoding sequences from the synganglion transcriptome of the cattle tick, Rhipicephalus microplus",
        "doc_id": "207840ed70e56993da252f8108c0595e",
        "_score": 129.3796,
        "objective_sentences": "We present Prediction of G protein-coupled receptor encoding sequences from the synganglion transcriptome of the cattle tick, Rhipicephalus microplus. We present an analysis of the R. microplus synganglion transcriptome using a combination of structurally-based and alignment-free methods which supplement the identification of GPCRs by sequence similarity.",
        "sem_score": 3.25
    },
    {
        "date": "2015-01-01",
        "raw_text": "We present Integrating alignment-based and alignment-free sequence similarity measures for biological sequence classification. Motivation: Alignment-based sequence similarity searches, while accurate for some type of sequences, can produce incorrect results when used on more divergent but functionally related sequences that have undergone the sequence rearrangements observed in many bacterial and viral genomes. Here, we propose a classification model that exploits the complementary nature of alignment-based and alignment-free similarity measures with the aim to improve the accuracy with which DNA and protein sequences are characterized.#R##N##R##N#Results: Our model classifies sequences using a combined sequence similarity score calculated by adaptively weighting the contribution of different sequence similarity measures. Weights are determined independently for each sequence in the test set and reflect the discriminatory ability of individual similarity measures in the training set. Because the similarity between some sequences is determined more accurately with one type of measure rather than another, our classifier allows different sets of weights to be associated with different sequences. Using five different similarity measures, we show that our model significantly improves the classification accuracy over the current composition- and alignment-based models, when predicting the taxonomic lineage for both short viral sequence fragments and complete viral sequences. We also show that our model can be used effectively for the classification of reads from a real metagenome dataset as well as protein sequences.#R##N##R##N#Availability and implementation: All the datasets and the code used in this study are freely available at https://collaborators.oicr.on.ca/vferretti/borozan_csss/csss.html.#R##N##R##N#Contact: moc.liamg@nazorob.navi#R##N##R##N#Supplementary information: Supplementary data are available at Bioinformatics online.",
        "title": "Integrating alignment-based and alignment-free sequence similarity measures for biological sequence classification",
        "doc_id": "05a076139058c0c5e9600458315f024e",
        "_score": 155.64961,
        "objective_sentences": "We present Integrating alignment-based and alignment-free sequence similarity measures for biological sequence classification. Here, we propose a classification model that exploits the complementary nature of alignment-based and alignment-free similarity measures with the aim to improve the accuracy with which DNA and protein sequences are characterized.#R##N##R##N#Results: Our model classifies sequences using a combined sequence similarity score calculated by adaptively weighting the contribution of different sequence similarity measures.",
        "sem_score": 3.1611111111111114
    },
    {
        "date": "2017-01-01",
        "raw_text": "We present Imparting Interpretability to Word Embeddings while Preserving Semantic Structure. As an ubiquitous method in natural language processing, word embeddings are extensively employed to map semantic properties of words into a dense vector representation. They capture semantic and syntactic relations among words but the vector corresponding to the words are only meaningful relative to each other. Neither the vector nor its dimensions have any absolute, interpretable meaning. We introduce an additive modification to the objective function of the embedding learning algorithm that encourages the embedding vectors of words that are semantically related to a predefined concept to take larger values along a specified dimension, while leaving the original semantic learning mechanism mostly unaffected. In other words, we align words that are already determined to be related, along predefined concepts. Therefore, we impart interpretability to the word embedding by assigning meaning to its vector dimensions. The predefined concepts are derived from an external lexical resource, which in this paper is chosen as Roget's Thesaurus. We observe that alignment along the chosen concepts is not limited to words in the Thesaurus and extends to other related words as well. We quantify the extent of interpretability and assignment of meaning from our experimental results. We also demonstrate the preservation of semantic coherence of the resulting vector space by using word-analogy and word-similarity tests. These tests show that the interpretability-imparted word embeddings that are obtained by the proposed framework do not sacrifice performances in common benchmark tests. ",
        "title": "Imparting Interpretability to Word Embeddings while Preserving Semantic Structure",
        "doc_id": "8ed12e51bfc1e669282f877a4ec45190",
        "_score": 123.44931,
        "objective_sentences": "We present Imparting Interpretability to Word Embeddings while Preserving Semantic Structure. We introduce an additive modification to the objective function of the embedding learning algorithm that encourages the embedding vectors of words that are semantically related to a predefined concept to take larger values along a specified dimension, while leaving the original semantic learning mechanism mostly unaffected.",
        "sem_score": 3.1400560224089635
    },
    {
        "date": "2015-01-01",
        "raw_text": "We present Similarity Calculation Method of Chinese Short Text Based on Semantic Feature Space. In order to improve the accuracy of short text similarity calculation, this paper presents the idea that use the history of short text messages to construct semantic feature space, then use the vector in semantic feature space to represent short text and do semantic extension, and finally calculate the short text similarity of corresponding vector in the semantic feature space. This method can represent the semantic information of short text message thoroughly so as to improve the accuracy of similarity calculation. We selected a large number of problem test sets for experiments. The results show that the method we proposed is reasonable and effective. I. INTRODUCTION With the wide application of short text similarity calculation method in information retrieval, question- answering system, text mining and other natural language processing fields, the research and improvement on the calculation method of short text similarity has become an important research hotspot. The research finds that there are many differences between the calculation methods of short text similarity and document similarity. As the document contains large amount of word information, most of the similarity calculation method is based on word statistical method. However, the short text contains little word information, maybe even only one word. It is not sufficient to judge the similarity between the short texts accurately only using the information of the short text itself. Therefore, in order to improve the calculation accuracy of short text similarity, we need to solve two key problems. The first problem is how to fully expressed and reflected short text information? The information includes word frequency, word meaning, etc. The second problem is how to calculate the similarity between the short texts? In order to solve these two problems, this paper presents the calculation method of Chinese short text semantic similarity based on the semantic feature space. This method represent the semantic information of short text message thoroughly so as to improve the accuracy of similarity calculation. We selected a large number of problem test sets for experiments. The results show that the method we proposed is reasonable and effective. II. CONSTRUCTION METHOD OF SEMANTIC FEATURE SPACE We take the intelligent-service system as the research background. The main short texts in the system are advisory information (namely interrogative sentences) and response short texts. In the intelligent service system, there are many users asking for advices every day, which inevitably produces massive consultation information. We can use these historical advisory information, namely short text sets to construct the semantic feature space, and then build the model by using the new consultation of the users or questioning short text in the space, finally we can calculate the similarity between the new short text and historical short text. The semantic feature space has a similar construction process with the ordinary vector space, which also consists of two main steps: feature selection and feature dimension reduction.",
        "title": "Similarity Calculation Method of Chinese Short Text Based on Semantic Feature Space",
        "doc_id": "9d8f3f29e30cfed78094fdfd96754434",
        "_score": 126.98529,
        "objective_sentences": "We present Similarity Calculation Method of Chinese Short Text Based on Semantic Feature Space. In order to improve the accuracy of short text similarity calculation, this paper presents the idea that use the history of short text messages to construct semantic feature space, then use the vector in semantic feature space to represent short text and do semantic extension, and finally calculate the short text similarity of corresponding vector in the semantic feature space.",
        "sem_score": 3.1116138763197587
    },
    {
        "date": "2019-10-01",
        "raw_text": "We present The impact of the image processing in the indexation system. This paper presents an efficient word spotting system applied to handwritten Arabic documents, where images are represented with bag-of-visual-SIFT descriptors and a sliding window approach is used to locate the regions that are most similar to the query by following the query-by-example paragon. First, a pre-processing step is used to produce a better representation of the most informative features. Secondly, a region-based framework is deployed to represent each local region by a bag-of-visual-SIFT descriptors. Afterward, some experiments are in order to demonstrate the codebook size influence on the efficiency of the system, by analyzing the curse of dimensionality curve. In the end, to measure the similarity score, a floating distance based on the descriptor\u2019s number for each query is adopted. The experimental results prove the efficiency of the proposed processing steps in the word spotting system.",
        "title": "The impact of the image processing in the indexation system",
        "doc_id": "49b369e8c5b5db34877497987b5e9077",
        "_score": 113.12327,
        "objective_sentences": "We present The impact of the image processing in the indexation system. This paper presents an efficient word spotting system applied to handwritten Arabic documents, where images are represented with bag-of-visual-SIFT descriptors and a sliding window approach is used to locate the regions that are most similar to the query by following the query-by-example paragon.",
        "sem_score": 3.0
    },
    {
        "date": "2011-01-01",
        "raw_text": "We present TopicView: Visually Comparing Topic Models of Text Collections. We present Topic View, an application for visually comparing and exploring multiple models of text corpora. Topic View uses multiple linked views to visually analyze both the conceptual content and the document relationships in models generated using different algorithms. To illustrate Topic View, we apply it to models created using two standard approaches: Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). Conceptual content is compared through the combination of (i) a bipartite graph matching LSA concepts with LDA topics based on the cosine similarities of model factors and (ii) a table containing the terms for each LSA concept and LDA topic listed in decreasing order of importance. Document relationships are examined through the combination of (i) side-by-side document similarity graphs, (ii) a table listing the weights for each document's contribution to each concept/topic, and (iii) a full text reader for documents selected in either of the graphs or the table. We demonstrate the utility of Topic View's visual approach to model assessment by comparing LSA and LDA models of two example corpora.",
        "title": "TopicView: Visually Comparing Topic Models of Text Collections",
        "doc_id": "05a64e26d3f80ab3306885bb85752ebe",
        "_score": 129.88565,
        "objective_sentences": "We present TopicView: Visually Comparing Topic Models of Text Collections. We present Topic View, an application for visually comparing and exploring multiple models of text corpora.",
        "sem_score": 3.0
    },
    {
        "date": "2011-01-01",
        "raw_text": "We present Integrative network alignment reveals large regions of global network similarity in yeast and human. Motivation: High-throughput methods for detecting molecular interactions have produced large sets of biological network data with much more yet to come. Analogous to sequence alignment, efficient and reliable network alignment methods are expected to improve our understanding of biological systems. Unlike sequence alignment, network alignment is computationally intractable. Hence, devising efficient network alignment heuristics is currently a foremost challenge in computational biology.#R##N##R##N#Results: We introduce a novel network alignment algorithm, called Matching-based Integrative GRAph ALigner (MI-GRAAL), which can integrate any number and type of similarity measures between network nodes (e.g. proteins), including, but not limited to, any topological network similarity measure, sequence similarity, functional similarity and structural similarity. Hence, we resolve the ties in similarity measures and find a combination of similarity measures yielding the largest contiguous (i.e. connected) and biologically sound alignments. MI-GRAAL exposes the largest functional, connected regions of protein\u2013protein interaction (PPI) network similarity to date: surprisingly, it reveals that 77.7% of proteins in the baker's yeast high-confidence PPI network participate in such a subnetwork that is fully contained in the human high-confidence PPI network. This is the first demonstration that species as diverse as yeast and human contain so large, continuous regions of global network similarity. We apply MI-GRAAL's alignments to predict functions of un-annotated proteins in yeast, human and bacteria validating our predictions in the literature. Furthermore, using network alignment scores for PPI networks of different herpes viruses, we reconstruct their phylogenetic relationship. This is the first time that phylogeny is exactly reconstructed from purely topological alignments of PPI networks.#R##N##R##N#Availability: Supplementary files and MI-GRAAL executables: http://bio-nets.doc.ic.ac.uk/MI-GRAAL/.#R##N##R##N#Contact: natasha@imperial.ac.uk#R##N##R##N#Supplementary information:Supplementary data are available at Bioinformatics online.",
        "title": "Integrative network alignment reveals large regions of global network similarity in yeast and human",
        "doc_id": "4222350d5f3b10736cdff8850125a79e",
        "_score": 141.88525,
        "objective_sentences": "We present Integrative network alignment reveals large regions of global network similarity in yeast and human. Hence, devising efficient network alignment heuristics is currently a foremost challenge in computational biology.#R##N##R##N#Results: We introduce a novel network alignment algorithm, called Matching-based Integrative GRAph ALigner (MI-GRAAL), which can integrate any number and type of similarity measures between network nodes (e.g. proteins), including, but not limited to, any topological network similarity measure, sequence similarity, functional similarity and structural similarity.",
        "sem_score": 2.838095238095238
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present Una aproximacin al uso de word embeddings en una tarea de similitud de textos en espaol. In this paper we show how a vector representation of words based on word embeddings can help to improve the results in tasks focused on the semantic similarity of texts. Thus we have experimented with two methods that rely on the vector representation of words to calculate the degree of similarity of two texts, one based on the aggregation of vectors and the other one based on the calculation of alignments. The alignment method relies on the similarity of word vectors to determine the semantic link between them. The aggregation method allows us to construct vector representations of the texts from the individual vectors of each word. These representations are compared by means of two classic distance measures: Euclidean distance and cosine similarity. We have evaluated our systems with the corpus based on Wikipedia distributed in the competition of similarity of texts in Spanish of SemEval-2015. Our experiments show that the method based on the alignment of words performs much better, obtaining results that are very close to the best system at SemEval. The method based on vector representations of texts behaves substantially worse. However, this second approach seems to capture aspects of similarity not detected by the first one, as when the outputs of both systems are combined the results of the alignment method are surpassed, even exceeding the results of the best system at SemEval.",
        "title": "Una aproximacin al uso de word embeddings en una tarea de similitud de textos en espaol",
        "doc_id": "5e1bd6fba41c05f6e17e36045daa8676",
        "_score": 146.2242,
        "objective_sentences": "We present Una aproximacin al uso de word embeddings en una tarea de similitud de textos en espaol. In this paper we show how a vector representation of words based on word embeddings can help to improve the results in tasks focused on the semantic similarity of texts.",
        "sem_score": 2.833333333333333
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present Una aproximacin al uso de word embeddings en una tarea de similitud de textos en espaol. In this paper we show how a vector representation of words based on word embeddings can help to improve the results in tasks focused on the semantic similarity of texts. Thus we have experimented with two methods that rely on the vector representation of words to calculate the degree of similarity of two texts, one based on the aggregation of vectors and the other one based on the calculation of alignments. The alignment method relies on the similarity of word vectors to determine the semantic link between them. The aggregation method allows us to construct vector representations of the texts from the individual vectors of each word. These representations are compared by means of two classic distance measures: Euclidean distance and cosine similarity. We have evaluated our systems with the corpus based on Wikipedia distributed in the competition of similarity of texts in Spanish of SemEval-2015. Our experiments show that the method based on the alignment of words performs much better, obtaining results that are very close to the best system at SemEval. The method based on vector representations of texts behaves substantially worse. However, this second approach seems to capture aspects of similarity not detected by the first one, as when the outputs of both systems are combined the results of the alignment method are surpassed, even exceeding the results of the best system at SemEval.",
        "title": "Una aproximacin al uso de word embeddings en una tarea de similitud de textos en espaol",
        "doc_id": "5e1bd6fba41c05f6e17e36045daa8676",
        "_score": 146.2242,
        "objective_sentences": "We present Una aproximacin al uso de word embeddings en una tarea de similitud de textos en espaol. In this paper we show how a vector representation of words based on word embeddings can help to improve the results in tasks focused on the semantic similarity of texts.",
        "sem_score": 2.833333333333333
    },
    {
        "date": "2015-01-01",
        "raw_text": "We present Textual Spatial Cosine Similarity. When dealing with document similarity many methods exist today, like cosine similarity. More complex methods are also available based on the semantic analysis of textual information, which are computationally expensive and rarely used in the real time feeding of content as in enterprise- wide search environments. To address these real-time constraints, we developed a new measure of document similarity called Textual Spatial Cosine Similarity, which is able to detect similitude at the semantic level using word placement information contained in the document. We will see in this paper that two degenerate cases exist for this model, which coincide with Cosine Similarity on one side and with a paraphrasing detection model to the other.",
        "title": "Textual Spatial Cosine Similarity",
        "doc_id": "745b122d3013e6135856eed35f88b42d",
        "_score": 139.23094,
        "objective_sentences": "We present Textual Spatial Cosine Similarity. To address these real-time constraints, we developed a new measure of document similarity called Textual Spatial Cosine Similarity, which is able to detect similitude at the semantic level using word placement information contained in the document.",
        "sem_score": 2.669047619047619
    },
    {
        "date": "2018-01-01",
        "raw_text": "We present A Tri-Partite Neural Document Language Model for Semantic Information Retrieval. Previous work in information retrieval have shown that using evidence, such as concepts and relations, from external knowledge resources could enhance the retrieval performance. Recently, deep neural approaches have emerged as state-of-the art models for capturing word semantics that can also be efficiently injected in IR models. This paper presents a new tri-partite neural document language framework that leverages explicit knowledge to jointly constrain word, concept, and document learning representations to tackle a number of issues including polysemy and granularity mismatch. We show the effectiveness of the framework in various IR tasks including document similarity, document re-ranking, and query expansion.",
        "title": "A Tri-Partite Neural Document Language Model for Semantic Information Retrieval",
        "doc_id": "d9c4533a24fbe77518607fa9cc36ee2b",
        "_score": 125.90307,
        "objective_sentences": "We present A Tri-Partite Neural Document Language Model for Semantic Information Retrieval. This paper presents a new tri-partite neural document language framework that leverages explicit knowledge to jointly constrain word, concept, and document learning representations to tackle a number of issues including polysemy and granularity mismatch.",
        "sem_score": 2.6333333333333333
    },
    {
        "date": "2017-01-01",
        "raw_text": "We present Word importance-based similarity of documents metric (WISDM): Fast and scalable document similarity metric for analysis of scientific documents. We present the Word importance-based similarity of documents metric (WISDM), a fast and scalable novel method for document similarity/distance computation for analysis of scientific documents. It is based on recent advancements in the area of word embeddings. WISDM combines learned word vectors together with traditional count-based models for document similarity computation, eventually achieving state-of-the-art performance and precision. The novel method first selects from two text documents those words that carry the most information and forms a word set for each document respectively. Then it relies on an existing word embeddings model to get the vector representations of the selected words. In the final step, it computes the closeness of the two sets of word vector representations, fit into a matrix, using a correlation coefficient. The presented metric was evaluated on three tasks, relevant to the analysis of scientific documents, and three data sets of open access scientific research. The results demonstrate that WISDM achieves significant performance speed-up in comparison to state-of-the-art metrics with a very marginal drop in precision.",
        "title": "Word importance-based similarity of documents metric (WISDM): Fast and scalable document similarity metric for analysis of scientific documents",
        "doc_id": "5acffe87501447404bc2227360a9f20c",
        "_score": 131.51291,
        "objective_sentences": "We present Word importance-based similarity of documents metric (WISDM): Fast and scalable document similarity metric for analysis of scientific documents. We present the Word importance-based similarity of documents metric (WISDM), a fast and scalable novel method for document similarity/distance computation for analysis of scientific documents.",
        "sem_score": 2.6
    },
    {
        "date": "2015-01-01",
        "raw_text": "We present Fine-Tuning an Algorithm for Semantic Search Using a Similarity Graph. Given a set of documents and an input query that is expressed in a natural language, the problem of document search is retrieving the most relevant documents. Unlike most existing systems that perform document search based on keyword matching, we propose a method that considers the meaning of the words in the queries and documents. As a result, our algorithm can return documents that have no words in common with the input query as long as the documents are relevant. For example, a document that contains the words \"Ford\", \"Chrysler\" and \"General Motors\" multiple times is surely relevant for the query \"car\" even if the word \"car\" never appears in the document. Our information retrieval algorithm is based on a similarity graph that contains the degree of semantic closeness between terms, where a term can be a word or a phrase. Since the algorithms that constructs the similarity graph takes as input a myriad of parameters, in this paper we fine-tune the part of the algorithm that constructs the Wikipedia part of the graph. Specifically, we experimentally fine-tune the algorithm on the Miller and Charles study benchmark that contains 30 pairs of terms and their similarity score as determined by human users. We then evaluate the performance of the fine-tuned algorithm on the Cranfield benchmark that contains 1400 documents and 225 natural language queries. The benchmark also contains the relevant documents for every query as determined by human judgment. The results show that the fine-tuned algorithm produces higher mean average precision (MAP) score than traditional keyword-based search algorithms because our algorithm considers not only the words and phrases in the query and documents, but also their meaning.",
        "title": "Fine-Tuning an Algorithm for Semantic Search Using a Similarity Graph",
        "doc_id": "fd7c167601220afd7f9a1a4e6780a3d4",
        "_score": 124.52871,
        "objective_sentences": "We present Fine-Tuning an Algorithm for Semantic Search Using a Similarity Graph. Unlike most existing systems that perform document search based on keyword matching, we propose a method that considers the meaning of the words in the queries and documents.",
        "sem_score": 2.585858585858586
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present A Chinese text paraphrase detection method based on dependency tree. Paraphrase detection is regarded as an important subtask in lots of natural language processing tasks. For example, in question answering, finding similar relations between questions needs paraphrase detection, also it is widely used in information retrieval, machine translation, document clustering, etc. Traditional solutions are mainly divided to two types. One is based on bag of words, which only considers the words in the sentences and similarity degrees between words. The other type is based on word embedding and deep neural networks, which learns word vectors to sentence vectors in deep models, in these models, deep layers may represent deep information in a sentence like phrase information and syntactic information, but these models may also lose some sentence information. We proposed a new method that considers word similarity and also directly uses dependency relations in sentences. We train our model in a Chinese text corpus. By working out dependency relation similarities and word similarities, we decide whether a sentence is a paraphrase of another one.",
        "title": "A Chinese text paraphrase detection method based on dependency tree",
        "doc_id": "2cb736f684684a9323e6a56dbfeadcf7",
        "_score": 115.81538,
        "objective_sentences": "We present A Chinese text paraphrase detection method based on dependency tree. We proposed a new method that considers word similarity and also directly uses dependency relations in sentences.",
        "sem_score": 2.571428571428571
    },
    {
        "date": "2015-01-01",
        "raw_text": "We present Practical issues in developing semantic frameworks for the analysis of verbal fluency data: A Norwegian data case study. Background: Verbal fluency tasks, which require producing as many words in response to a cue in a fixed time, are widely used within clinical neuropsychology and in neuropsychological research. Although semantic word lists can be elicited, typically only the number of words related to the cue is interpreted thus ignoring any structure in the word sequences. Automated language techniques can provide a much needed framework for extracting and charting useful semantic relations in healthy individuals and understanding how cortical disorders disrupt these knowledge structures and the retrieval of information from them. Methods: One minute, animal category verbal fluency tests from 150 participants consisting of healthy individuals, patients with schizophrenia, and patients with bipolar disorder were transcribed. We discuss the issues involved in building and evaluating semantic frameworks and developing robust features to analyze this data. Specifically we investigate a Latent Semantic Analysis (LSA) semantic space to obtain semantic features, such as pairwise semantic similarity and clusters. Results and Discussion: An in-depth analysis of the framework is presented, and then results from two measures based on LSA semantic similarity illustrate how these automated techniques provide additional, clinically useful information beyond word list cardinality.",
        "title": "Practical issues in developing semantic frameworks for the analysis of verbal fluency data: A Norwegian data case study",
        "doc_id": "96562d87142af5f703845baad2162435",
        "_score": 141.23796,
        "objective_sentences": "We present Practical issues in developing semantic frameworks for the analysis of verbal fluency data: A Norwegian data case study. We discuss the issues involved in building and evaluating semantic frameworks and developing robust features to analyze this data.",
        "sem_score": 2.571428571428571
    },
    {
        "date": "2013-07-15",
        "raw_text": "We present An Algorithm to Compute Protein Homology Based On Hydrophobic Cluster Analysis. AbstractCurrent techniques in protein homology testing involve a 1-dimensional alignment of Nucleotide or Amino acid sequencing. Due to its various constraints and low sequence identity values, a 2-Dimensional Hydrophobic Cluster Alignment has increasingly been used to predict the structure and functionality of protein. This work proposed an algorithm based on a secondary-structure Hydrophobic Cluster Alignment to compute a similarity score of protein sequences automatically, which helps reduce interventions of a human expert for a manual alignment. Additional techniques are introduced to speed up the calculation, as well as to resolve some greedy-based alignment limitation in the previous work. The alignment results and the classification accuracies from the well-known HOMSTRAD database have demonstrated an improvement in both accuracy and the computation time. Keywords-bioinformatic; hydrophobic cluster analysis; protien homology; automatic alignment I.",
        "title": "An Algorithm to Compute Protein Homology Based On Hydrophobic Cluster Analysis",
        "doc_id": "0d59287b038983939148cc37f61af2ff",
        "_score": 134.95862,
        "objective_sentences": "We present An Algorithm to Compute Protein Homology Based On Hydrophobic Cluster Analysis. This work proposed an algorithm based on a secondary-structure Hydrophobic Cluster Alignment to compute a similarity score of protein sequences automatically, which helps reduce interventions of a human expert for a manual alignment.",
        "sem_score": 2.5648351648351646
    },
    {
        "date": "2017-01-01",
        "raw_text": "We present Topic Model Based Text Similarity Measure for Chinese Judgment Document. In the recent informatization of Chinese courts, the huge amount of law cases and judgment documents, which were digital stored, has provided a good foundation for the research of judicial big data and machine learning. In this situation, some ideas about Chinese courts can reach automation or get better result through the research of machine learning, such as similar documents recommendation, workload evaluation based on similarity of judgement documents and prediction of possible relevant statutes. In trying to achieve all above mentioned, and also in face of the characteristics of Chinese judgement document, we propose a topic model based approach to measure the text similarity of Chinese judgement document, which is based on TF-IDF, Latent Dirichlet Allocation (LDA), Labeled Latent Dirichlet Allocation (LLDA) and other treatments. Combining with the characteristics of Chinese judgment document, we focus on the specific steps of approach, the preprocessing of corpus, the parameters choices of training and the evaluation of similarity measure result. Besides, implementing the approach for prediction of possible statutes and regarding the prediction accuracy as the evaluation metric, we designed experiments to demonstrate the reasonability of decisions in the process of design and the high performance of our approach on text similarity measure. The experiments also show the restriction of our approach which need to be focused in future work.",
        "title": "Topic Model Based Text Similarity Measure for Chinese Judgment Document",
        "doc_id": "cf50d9e9a212daf44668e702c603230a",
        "_score": 121.89822,
        "objective_sentences": "We present Topic Model Based Text Similarity Measure for Chinese Judgment Document. In trying to achieve all above mentioned, and also in face of the characteristics of Chinese judgement document, we propose a topic model based approach to measure the text similarity of Chinese judgement document, which is based on TF-IDF, Latent Dirichlet Allocation (LDA), Labeled Latent Dirichlet Allocation (LLDA) and other treatments.",
        "sem_score": 2.5
    },
    {
        "date": "2017-01-01",
        "raw_text": "We present DT_Team at SemEval-2017 Task 1: Semantic Similarity Using Alignments, Sentence-Level Embeddings and Gaussian Mixture Model Output. We describe our system (DT Team) submitted at SemEval-2017 Task 1, Semantic Textual Similarity (STS) challenge for English (Track 5). We developed three different models with various features including similarity scores calculated using word and chunk alignments, word/sentence embeddings, and Gaussian Mixture Model (GMM). The correlation between our systems output and the human judgments were up to 0.8536, which is more than 10% above baseline, and almost as good as the best performing system which was at 0.8547 correlation (the difference is just about 0.1%). Also, our system produced leading results when evaluated with a separate STS benchmark dataset. The word alignment and sentence embeddings based features were found to be very effective.",
        "title": "DT_Team at SemEval-2017 Task 1: Semantic Similarity Using Alignments, Sentence-Level Embeddings and Gaussian Mixture Model Output",
        "doc_id": "2081c0d3180af9eea4751ae17c50b505",
        "_score": 127.342705,
        "objective_sentences": "We present DT_Team at SemEval-2017 Task 1: Semantic Similarity Using Alignments, Sentence-Level Embeddings and Gaussian Mixture Model Output. We developed three different models with various features including similarity scores calculated using word and chunk alignments, word/sentence embeddings, and Gaussian Mixture Model (GMM).",
        "sem_score": 2.466666666666667
    },
    {
        "date": "2012-01-01",
        "raw_text": "We present Text-to-Text Similarity of Sentences. Assessing the semantic similarity between two texts is a central task in many applications, including summarization, intelligent tutoring systems, and software testing. Similarity of texts is typically explored at the level of word, sentence, paragraph, and document. The similarity can be defined quantitatively (e.g. in the form of a normalized value between 0 and 1) and qualitatively in the form of semantic relations such as elaboration, entailment, or paraphrase. In this chapter, we focus first on measuring quantitatively and then on detecting qualitatively sentence-level text-to-text semantic relations. A generic approach that relies on word-to-word similarity measures is presented as well as experiments and results obtained with various instantiations of the approach. In addition, we provide results of a study on the role of weighting in Latent Semantic Analysis, a statistical technique to assess similarity of texts. The results were obtained on two data sets: a standard data set on sentence-level paraphrase detection and a data set from an intelligent tutoring system.",
        "title": "Text-to-Text Similarity of Sentences",
        "doc_id": "dd8aabe504e0723dac23dc12ed958902",
        "_score": 148.53098,
        "objective_sentences": "We present Text-to-Text Similarity of Sentences. In this chapter, we focus first on measuring quantitatively and then on detecting qualitatively sentence-level text-to-text semantic relations.",
        "sem_score": 2.466666666666667
    },
    {
        "date": "2015-01-01",
        "raw_text": "We present Evaluation of Lexical-Based Approaches to the Semantic Similarity of Malay Sentences. AbstractWe evaluate existing and modified approaches for measuring the semantic similarity of sentences in the Malay language. These approaches are mainly used for English sentences and no studies to date have evaluated and compared their effectiveness when applied to Malay sentences. We used a pre-processed Malay machine-readable dictionary to calculate word-to-word semantic similarity with two methods: probability of intersection and normalization. We then used the word-to-word semantic similarity measure to identify semantic sentence similarity. We evaluated five measures of semantic sentence similarity: vector-based semantic similarity, word order similarity, highest word-to-sentence similarity, and combinations of vector-based and word-to-sentence similarity and of word order and word-to-sentence similarity. We also evaluated the effects of including and excluding lexical components such as prepositions, conjunctions, verbs, and morphological variants.",
        "title": "Evaluation of Lexical-Based Approaches to the Semantic Similarity of Malay Sentences",
        "doc_id": "592e8a62a92198a0f202c3392f435b70",
        "_score": 130.58978,
        "objective_sentences": "We present Evaluation of Lexical-Based Approaches to the Semantic Similarity of Malay Sentences. We used a pre-processed Malay machine-readable dictionary to calculate word-to-word semantic similarity with two methods: probability of intersection and normalization.",
        "sem_score": 2.466666666666667
    },
    {
        "date": "2013-01-01",
        "raw_text": "We present Document clustering on Hierarchical Methods Clustering with Multi viewpoint-Based Similarity Measure. Clustering is a division of data into groups of similar objects. Representing the data by fewer clusters necessarily loses certain fine details, but achieves simplification. The similar documents are grouped together in a cluster, if their cosine similarity measure is less than a specified threshold. In this paper we mainly focuses on document clustering and measures in hierarchical clustering. The hierarchical document clustering algorithm provides a natural way of distinguishing clusters and implementing the basic requirement of clustering as high within-cluster similarity and between-cluster dissimilarity KEY TermsDocument clustering, text mining, similarity measure .Hierarchical Methods INTRODUCTION Document clustering is automatic document organization, topic extraction and fast information retrieval or filtering. It is closely related to data clustering. Document clustering techniques mostly rely on single term analysis of the document data set, such as the Vector Space Model. To achieve more accurate document clustering, more informative features including phrases and their weights are particularly important in such scenarios. Document clustering involves the use of descriptors and descriptor extraction. Descriptors are sets of words that describe the contents within the cluster. Document clustering is generally considered to be a centralized process. Document clustering is particularly useful in many applications such as automatic categorization of documents, grouping search engine results, building taxonomy of documents, and others. For this Hierarchical Clustering method provides a better improvement in achieving the result. Our project presents two key parts of successful Hierarchical document clustering. The first part is a document index model, the Document Index Graph, which allows for incremental construction of the index of the document set with an emphasis on efficiency, rather than relying on single-term indexes only. It provides efficient phrase matching that is used to judge the similarity between documents. This model is flexible in that it could revert to a compact representation of the vector space model if we choose not to index phrases. The second part is an incremental document clustering algorithm based on maximizing the tightness of clusters by carefully watching the pairwise document similarity distribution inside clusters. Existing Systems greedily picks the next frequent item set which represent the next cluster to minimize the overlapping between the documents that contain both the item set and some remaining item sets. The clustering result depends on the order of picking up the item sets, which in turns depends on the greedy heuristic. This method does not follow a sequential order of selecting clusters. Instead, we assign documents to the best cluster. In proposed approach, The main work is to develop a novel hierarchal algorithm for document clustering which provides maximum efficiency and performance. It is particularly focused in studying and making use of cluster overlapping phenomenon to design cluster merging criteria. Proposing a new way to compute the overlap rate in order to improve time efficiency and the veracity is mainly concentrated. Based on the Hierarchical Clustering Method, the usage of Expectation-Maximization (EM) algorithm in the Gaussian Mixture Model to count the parameters and make the two sub-clusters (IJIRSE) International Journal of Innovative Research in Science & Engineering ISSN (Online) 2347-3207 combined when their overlap is the largest is narrated. Experiments in both public data and document clustering data show that this approach can improve the efficiency of clustering and save computing time.Hierarchical techniques produce a nested sequence of partitions, with a single, all inclusive cluster at the top and singleton clusters of individual points at the bottom. Each intermediate level can be viewed as combining two clusters from the next lower level (or splitting a cluster from the next higher level). The result of a hierarchical clustering algorithm can be graphically displayed as tree, called a dendogram. This tree graphically displays the merging process and the intermediate clusters. The dendogram at the right shows how four points can be merged into a single cluster. For document clustering, this dendogram provides a taxonomy, or hierarchical index. Fig;1 CHALLENGES IN HIERARCHICAL DOCUMENT CLUSTERING: A. High dimensionality Each distinct word in the document set constitutes a dimension. So there may be 15~20 thousands dimensions. This type of high\\dimensionality greatly affects the scalability and efficiency of many existing clustering algorithms . B. High volume of data In text mining, processing of data about 10 thousands to 100 thousands documents are involved. C. Consistently high accuracy: Some existing algorithms only work fine for certain type of document sets, but may not perform well in some others. D. Meaningful cluster description: This is important for the end user. The resulting hierarchy should facilitate browsing. HIERARCHICAL ANALYSIS MODEL A hierarchical clustering algorithm creates a hierarchical decomposition of the given set of data objects. Depending on the decomposition approach, hierarchical algorithms are classified as agglomerative (merging) or divisive (splitting) A. Agglomerative: Start with the points as individual clusters and, at each step, merge the most similar or closest pair of clusters. This requires a definition of cluster similarity or distance B. Divisive: Start with one, all-inclusive cluster and, at each step, split a cluster until only singleton clusters of individual points remain. In this case, we need to decide, at each step, which cluster to split and how to perform the split. Agglomerative techniques are more common, and these are the techniques that we will compare to Kmeans and its variants. agglomerative hierarchical clustering procedure as follows: Simple Agglomerative Clustering Algorithm 1. Compute the similarity between all pairs of clusters, i.e., calculate a similarity matrix whose ijth entry gives the similarity between the ith and jth clusters. 2. Merge the most similar (closest) two clusters. 3. Update the similarity matrix to reflect the pairwise similarity between the new (IJIRSE) International Journal of Innovative Research in Science & Engineering ISSN (Online) 2347-3207 cluster and the original clusters. 4. Repeat steps 2 and 3 until only a single cluster remains. Fig:2. Hierarchical Clustering STEP 1 Start by assigning each item to a cluster, so that if you have N items, you now have N clusters, each containing just one item. Let the distances (similarities) between the clusters the same as the distances (similarities) between the items they contain . STEP 2 Find the closest (most similar) pair of clusters and merge them into a single cluster, so that now you have one cluster less with the help of tf idf. STEP 3 Compute distances (similarities) between the new cluster and each of the old clusters. STEP 4 Repeat steps 2 and 3 until all items are clustered into a single cluster of size N. Step 3 can be done in different ways, which is what distinguishes single-linkage from complete linkage and average-linkage clustering. In single linkage clustering (also called the connectedness or minimum method), considering the distance between one cluster and another cluster to be equal to the shortest distance from any member of one cluster to any member of the other cluster. If the data consist of similarities consider the similarity between one cluster and another cluster to be equal to the greatest similarity from any member of one cluster to any member of the other cluster. In complete linkage clustering (also called the diameter or maximum method), consider the distance between one cluster and another cluster to be equal to the greatest distance from any member of one cluster to any member of the other cluster. In average-linkage clustering, consider the distance between one cluster and another cluster to be equal to the average distance. This kind of hierarchical clustering is called agglomerative because it merges clusters iteratively. Divisive hierarchical clustering which does the reverse by starting with all objects in one cluster and subdividing them into smaller pieces. Divisive methods are not generally available, and rarely have been applied. Of course there is no point in having all the N items grouped in a single cluster but, once the complete hierarchical tree is obtained and need k clusters, k-1 longest links are eliminated. Techniques: Intra-Cluster Similarity Technique (IST): This hierarchical technique looks at the similarity of all the documents in a cluster to their cluster centroid and is defined by where d is a document in cluster, X, and c is the centroid of cluster X. The choice of which pair of clusters to merge is made by determining which pair of clusters will lead to smallest decrease in similarity. Thus, if cluster Z is formed by merging clusters X and Y, then we select X and Y so as to maximize Sim(Z) (Sim(X) + Sim(Y)). Note that Sim(Z) (Sim(X)+Sim(Y)) is non-positive. Centroid Similarity Technique (CST): This hierarchical technique defines the similarity of two clusters to be the cosine similarity between the centroids of the two clusters. UPGMA: It defines the cluster similarity as follows (IJIRSE) International Journal of Innovative Research in Science & Engineering ISSN (Online) 2347-3207 where d1 and d2 are, documents, respectively, in cluster1 and cluster2. TERM FREQUENCY -INVERSE DOCUMENT FREQUENCY The TF-IDF is a text statistical-based technique which has been widely used in many search engines and information retrieval systems. Assume that there is a corpora of 1000 documents and the task is to compute the similarity between two given documents (or a document and a query). The following describes the steps of acquiring the simila",
        "title": "Document clustering on Hierarchical Methods Clustering with Multi viewpoint-Based Similarity Measure",
        "doc_id": "c7b39b694865aeb3a97d2aa691bacdc9",
        "_score": 130.30894,
        "objective_sentences": "We present Document clustering on Hierarchical Methods Clustering with Multi viewpoint-Based Similarity Measure. In this paper we mainly focuses on document clustering and measures in hierarchical clustering.",
        "sem_score": 2.4444444444444446
    },
    {
        "date": "2013-01-01",
        "raw_text": "We present Ranking of web documents using semantic similarity. In recent years, semantic search for relevant documents on web has been an important topic of research. Many semantic web search engines have been developed like Ontolook, Swoogle, etc that helps in searching meaningful documents presented on semantic web. The concept of semantic similarity has been widely used in many fields like artificial intelligence, cognitive science, natural language processing, psychology. To relate entities/texts/documents having same meaning, semantic similarity approach is used based on matching of the keywords which are extracted from the documents using syntactic parsing. The simple lexical matching usually used by semantic search engine does not extract web documents to the user expectations. In this paper we have proposed a ranking scheme for the semantic web documents by finding the semantic similarity between the documents and the query which is specified by the user. The novel approach proposed in this paper not only relies on the syntactic structure of the document but also considers the semantic structure of the document and the query. The approach used here includes the lexical as well as the conceptual matching. The combined use of conceptual, linguistic and ontology based matching has significantly improved the performance of the proposed ranking scheme. We explore all relevant relations between the keywords exploring the user's intention and then calculate the fraction of these relations on each web page to determine their relevance with respect to the query provided by the user. We have found that this semantic similarity based ranking scheme gives much better results than those by the prevailing methods.",
        "title": "Ranking of web documents using semantic similarity",
        "doc_id": "6ec232cae853ad574e4fe523b441dade",
        "_score": 123.089935,
        "objective_sentences": "We present Ranking of web documents using semantic similarity. In this paper we have proposed a ranking scheme for the semantic web documents by finding the semantic similarity between the documents and the query which is specified by the user.",
        "sem_score": 2.4333333333333336
    },
    {
        "raw_text": "We present Template-Based Modeling of Protein-RNA Interactions. Protein-RNA complexes formed by specific recognition between RNA and RNA-binding proteins play an important role in biological processes. More than a thousand of such proteins in human are curated and many novel RNA-binding proteins are to be discovered. Due to limitations of experimental approaches, computational techniques are needed for characterization of protein-RNA interactions. Although much progress has been made, adequate methodologies reliably providing atomic resolution structural details are still lacking. Although protein-RNA free docking approaches proved to be useful, in general, the template-based approaches provide higher quality of predictions. Templates are key to building a high quality model. Sequence/structure relationships were studied based on a representative set of binary protein-RNA complexes from PDB. Several approaches were tested for pairwise target/template alignment. The analysis revealed a transition point between random and correct binding modes. The results showed that structural alignment is better than sequence alignment in identifying good templates, suitable for generating protein-RNA complexes close to the native structure, and outperforms free docking, successfully predicting complexes where the free docking fails, including cases of significant conformational change upon binding. A template-based protein-RNA interaction modeling protocol PRIME was developed and benchmarked on a representative set of complexes. Author Summary Structures of protein-RNA complexes are important for characterization of biological processes. The number of experimentally determined protein-RNA complexes is limited. Thus modeling of these complexes is important. Reliable structural predictions of proteins and their complexes are provided by comparative modeling, which takes advantage of similar complexes with experimentally determined structures. Thus, in the case of protein-RNA complexes, it is important to determine if similar proteins and RNAs bind in a similar way. We show that, similarly to the earlier published results on protein-protein complexes, such correlation of the protein-RNA binding mode and the monomers similarity indeed exists, and is stronger when the similarity is determined by structure rather than sequence alignment. The data shows clear transition from random to similar binding mode with the increase of the structural similarity of the monomers. On the basis of the results we designed and implemented a predictive tool, which should be useful for the biological community interested in modeling of protein-RNA interactions.",
        "date": "2016-09-23",
        "title": "Template-Based Modeling of Protein-RNA Interactions",
        "doc_id": "070194c4fd19770d24c1434cba1bbafa",
        "_score": 110.198166,
        "objective_sentences": "We present Template-Based Modeling of Protein-RNA Interactions. We show that, similarly to the earlier published results on protein-protein complexes, such correlation of the protein-RNA binding mode and the monomers similarity indeed exists, and is stronger when the similarity is determined by structure rather than sequence alignment.",
        "sem_score": 2.3333333333333335
    },
    {
        "date": "2011-01-01",
        "raw_text": "We present Sequence Alignment as Hypothesis Testing. Sequence alignment depends on the scoring function that defines similarity between pairs of letters. For local alignment, the computational algorithm searches for the most similar segments in the sequences according to the scoring function. The choice of this scoring function is important for correctly detecting segments of interest. We formulate sequence alignment as a hypothesis testing problem, and conduct extensive simulation experiments to study the relationship between the scoring function and the distribution of aligned pairs within the aligned segment under this framework. We cut through the many ways to construct scoring functions and showed that any scoring function with negative expectation used in local alignment corresponds to a hypothesis test between the background distribution of sequence letters and a statistical distribution of letter pairs determined by the scoring function. The results indicate that the log-likelihood ratio scoring function is statistically most powerful and has the highest accuracy for detecting the segments of interest that are defined by the statistical distribution of aligned letter pairs.",
        "title": "Sequence Alignment as Hypothesis Testing",
        "doc_id": "d8c6963ca118fb7cc1d05bdf83e153cf",
        "_score": 124.6955,
        "objective_sentences": "We present Sequence Alignment as Hypothesis Testing. We formulate sequence alignment as a hypothesis testing problem, and conduct extensive simulation experiments to study the relationship between the scoring function and the distribution of aligned pairs within the aligned segment under this framework.",
        "sem_score": 2.333333333333333
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present Relaxing Orthogonality Assumption in Conceptual Text Document Similarity. By reflecting the degree of proximity or remoteness of documents, similarity measure plays the key role in text analytics. Traditional measures, e.g. cosine similarity, assume that documents are represented in an orthogonal space formed by words as dimensions.   Words are considered independent from each other and document similarity is computed based on lexical overlap. This assumption is also made in the bag of concepts representation of documents while the space is formed by concepts. This paper proposes new semantic similarity measures without relying on the orthogonality assumption. By employing Wikipedia as an external resource, we introduce five similarity measures using concept-concept relatedness. Experimental results on real text datasets reveal that eliminating the orthogonality assumption improves the quality of text clustering algorithms.",
        "title": "Relaxing Orthogonality Assumption in Conceptual Text Document Similarity",
        "doc_id": "24de6601d83d3cbe24174e4f5f0193af",
        "_score": 111.176094,
        "objective_sentences": "We present Relaxing Orthogonality Assumption in Conceptual Text Document Similarity. This paper proposes new semantic similarity measures without relying on the orthogonality assumption.",
        "sem_score": 2.3
    },
    {
        "date": "2013-08-01",
        "raw_text": "We present Measuring the structural similarity of web-based documents: A novel approach. Abstract  Most known methods for measuring the structural similarity of document structures are based on, e.g., tag measures, path metrics and tree measures in terms of their DOM-Trees. Other methods measures the similarity in the framework of the well known vector space model. In contrast to these we present a new approach to measuring the structural similarity of web-based documents represented by so called generalized trees which are more general than DOM-Trees which represent only directed rooted trees. We will design a new similarity measure for graphs representing web-based hypertext structures. Our similarity measure is mainly based on a novel representation of a graph as strings of linear integers, whose components represent structural properties of the graph. The similarity of two graphs is then defined as the optimal alignment of the underlying property strings. In this paper we apply the well known technique of sequence alignments to solve a novel and challenging problem: Measuring the structural similarity of generalized trees. More precisely, we first transform our graphs considered as high dimensional objects in linear structures. Then we derive similarity values from the alignments of the property strings in order to measure the structural similarity of generalized trees. Hence, we transform a graph similarity problem to a string similarity problem. We demonstrate that our similarity measure captures important structural information by applying it to two different test sets consisting of graphs representing web-based documents.",
        "title": "Measuring the structural similarity of web-based documents: A novel approach",
        "doc_id": "4be4629c2a1d1ae37f49050e038f6e5d",
        "_score": 152.44801,
        "objective_sentences": "We present Measuring the structural similarity of web-based documents: A novel approach. Abstract   In contrast to these we present a new approach to measuring the structural similarity of web-based documents represented by so called generalized trees which are more general than DOM-Trees which represent only directed rooted trees.",
        "sem_score": 2.3
    },
    {
        "date": "2013-01-01",
        "raw_text": "We present Paraphrase identification in short texts using grammar patterns. We can determine whether two texts are paraphrases of each other by finding out the extent to which the texts are similar. The typical lexical matching technique works by matching the sequence of tokens between the texts to recognize paraphrases, and fails when different words are used to convey the same meaning. We can improve this simple method by combining lexical with syntactic or semantic representations of the input texts. The present work makes use of syntactical information in the texts and computes the similarity between them using word similarity measures based on WordNet and lexical databases. The texts are converted into a unified semantic structural model through which the semantic similarity of the texts is obtained. An approach is presented to assess the semantic similarity and the results of applying this approach is evaluated using the Microsoft Research Paraphrase (MSRP) Corpus.",
        "title": "Paraphrase identification in short texts using grammar patterns",
        "doc_id": "a9d37c65d15e23d7669eab38b9362e86",
        "_score": 136.50197,
        "objective_sentences": "We present Paraphrase identification in short texts using grammar patterns. We can determine whether two texts are paraphrases of each other by finding out the extent to which the texts are similar.",
        "sem_score": 2.3
    },
    {
        "date": "2012-01-01",
        "raw_text": "We present 2D shapes classification using BLAST. This paper presents a novel 2D shape classification approach, which exploits in this context the huge amount of work carried out by bioinformaticians in the biological sequence analysis research field. In particular, in the approach presented here, we propose to encode shapes as biological sequences, employing the widely known sequence alignment tool called BLAST (Basic Local Alignment Search Tool) to devise a similarity score, used in a nearest neighbour scenario. Obtained results on standard datasets show the feasibility of the proposed approach.",
        "title": "2D shapes classification using BLAST",
        "doc_id": "2f311e663681d2675f7e784f384f4fba",
        "_score": 132.50412,
        "objective_sentences": "We present 2D shapes classification using BLAST. This paper presents a novel 2D shape classification approach, which exploits in this context the huge amount of work carried out by bioinformaticians in the biological sequence analysis research field.",
        "sem_score": 2.2818181818181817
    },
    {
        "raw_text": "We present Contextual Document Similarity for Content-based Literature Recommender Systems. To cope with the ever-growing information overload, an increasing number of digital libraries employ content-based recommender systems. These systems traditionally recommend related documents with the help of similarity measures. However, current document similarity measures simply distinguish between similar and dissimilar documents. This simplification is especially crucial for extensive documents, which cover various facets of a topic and are often found in digital libraries. Still, these similarity measures neglect to what facet the similarity relates. Therefore, the context of the similarity remains ill-defined. In this doctoral thesis, we explore contextual document similarity measures, i.e., methods that determine document similarity as a triple of two documents and the context of their similarity. The context is here a further specification of the similarity. For example, in the scientific domain, research papers can be similar with respect to their background, methodology, or findings. The measurement of similarity in regards to one or more given contexts will enhance recommender systems. Namely, users will be able to explore document collections by formulating queries in terms of documents and their contextual similarities. Thus, our research objective is the development and evaluation of a recommender system based on contextual similarity. The underlying techniques will apply established similarity measures and as well as neural approaches while utilizing semantic features obtained from links between documents and their text. ",
        "date": "2020-08-01",
        "title": "Contextual Document Similarity for Content-based Literature Recommender Systems",
        "doc_id": "f22c8d76dc73b2e91d9f0438570e7389",
        "_score": 131.7859,
        "objective_sentences": "We present Contextual Document Similarity for Content-based Literature Recommender Systems. In this doctoral thesis, we explore contextual document similarity measures, i.e., methods that determine document similarity as a triple of two documents and the context of their similarity.",
        "sem_score": 2.2
    },
    {
        "raw_text": "We present Representational Similarity Mapping of Distributional Semantics in Left Inferior Frontal, Middle Temporal, and Motor Cortex. Abstract Language comprehension engages a distributed network of frontotemporal, parietal, and sensorimotor regions, but it is still unclear how meaning of words and their semantic relationships are represented and processed within these regions and to which degrees lexico-semantic representations differ between regions and semantic types. We used fMRI and representational similarity analysis to relate word-elicited multivoxel patterns to semantic similarity between action and object words. In left inferior frontal (BA 44-45-47), left posterior middle temporal and left precentral cortex, the similarity of brain response patterns reflected semantic similarity among action-related verbs, as well as across lexical classes-between action verbs and tool-related nouns and, to a degree, between action verbs and food nouns, but not between action verbs and animal nouns. Instead, posterior inferior temporal cortex exhibited a reverse response pattern, which reflected the semantic similarity among object-related nouns, but not action-related words. These results show that semantic similarity is encoded by a range of cortical areas, including multimodal association (e.g., anterior inferior frontal, posterior middle temporal) and modality-preferential (premotor) cortex and that the representational geometries in these regions are partly dependent on semantic type, with semantic similarity among action-related words crossing lexical-semantic category boundaries.",
        "date": "2017-01-11",
        "title": "Representational Similarity Mapping of Distributional Semantics in Left Inferior Frontal, Middle Temporal, and Motor Cortex",
        "doc_id": "2e0082520efb959ca673e2bb4c9001f4",
        "_score": 113.68699,
        "objective_sentences": "We present Representational Similarity Mapping of Distributional Semantics in Left Inferior Frontal, Middle Temporal, and Motor Cortex. We used fMRI and representational similarity analysis to relate word-elicited multivoxel patterns to semantic similarity between action and object words.",
        "sem_score": 2.1666666666666665
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present Context Semantic Analysis: A Knowledge-Based Technique for Computing Inter-document Similarity. We propose a novel knowledge-based technique for inter-document similarity, called Context Semantic Analysis (CSA). Several specialized approaches built on top of specific knowledge base (e.g. Wikipedia) exist in literature but CSA differs from them because it is designed to be portable to any RDF knowledge base. Our technique relies on a generic RDF knowledge base (e.g. DBpedia and Wikidata) to extract from it a vector able to represent the context of a document. We show how such a Semantic Context Vector can be effectively exploited to compute inter-document similarity. Experimental results show that our general technique outperforms baselines built on top of traditional methods, and achieves a performance similar to the ones of specialized methods.",
        "title": "Context Semantic Analysis: A Knowledge-Based Technique for Computing Inter-document Similarity",
        "doc_id": "7e79ad788af1fe81f77cb24b66bb6028",
        "_score": 121.34751,
        "objective_sentences": "We present Context Semantic Analysis: A Knowledge-Based Technique for Computing Inter-document Similarity. We propose a novel knowledge-based technique for inter-document similarity, called Context Semantic Analysis (CSA).",
        "sem_score": 2.1333333333333333
    },
    {
        "raw_text": "We present AliGROOVE \u2013 visualization of heterogeneous sequence divergence within multiple sequence alignments and detection of inflated branch support. Background Masking of multiple sequence alignment blocks has become a powerful method to enhance the tree-likeness of the underlying data. However, existing masking approaches are insensitive to heterogeneous sequence divergence which can mislead tree reconstructions. We present AliGROOVE, a new method based on a sliding window and a Monte Carlo resampling approach, that visualizes heterogeneous sequence divergence or alignment ambiguity related to single taxa or subsets of taxa within a multiple sequence alignment and tags suspicious branches on a given tree. Results We used simulated multiple sequence alignments to show that the extent of alignment ambiguity in pairwise sequence comparison is correlated with the frequency of misplaced taxa in tree reconstructions. The approach implemented in AliGROOVE allows to detect nodes within a tree that are supported despite the absence of phylogenetic signal in the underlying multiple sequence alignment. We show that AliGROOVE equally well detects heterogeneous sequence divergence in a case study based on an empirical data set of mitochondrial DNA sequences of chelicerates. Conclusions The AliGROOVE approach has the potential to identify single taxa or subsets of taxa which show predominantly randomized sequence similarity in comparison with other taxa in a multiple sequence alignment. It further allows to evaluate the reliability of node support in a novel way. Electronic supplementary material The online version of this article (doi:10.1186/1471-2105-15-294) contains supplementary material, which is available to authorized users.",
        "date": "2014-08-30",
        "title": "AliGROOVE \u2013 visualization of heterogeneous sequence divergence within multiple sequence alignments and detection of inflated branch support",
        "doc_id": "3a2e8f01110cc0d3ba53f1b50a02e606",
        "_score": 126.34918,
        "objective_sentences": "We present AliGROOVE \u2013 visualization of heterogeneous sequence divergence within multiple sequence alignments and detection of inflated branch support. We present AliGROOVE, a new method based on a sliding window and a Monte Carlo resampling approach, that visualizes heterogeneous sequence divergence or alignment ambiguity related to single taxa or subsets of taxa within a multiple sequence alignment and tags suspicious branches on a given tree.",
        "sem_score": 2.1151515151515152
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present A Deep Learning Methodology for Semantic Utterance Classification in Virtual Human Dialogue Systems. This paper describes the development of a deep learning methodology for semantic utterance classification (SUC) for use in domainspecific dialogue systems. Semantic classifiers need to account for a variety of instances where the utterance for the semantic domain class varies. In order to capture the candidate relationships between the semantic class and the word sequence in an utterance, we have proposed a shallow convolutional neural network (CNN) along with a recurrent neural network (RNN) that uses domain-specific word embeddings which have been initialized using Word2Vec for determining semantic similarity of words. Experimental results demonstrate the effectiveness of shallow neural networks for SUC.",
        "title": "A Deep Learning Methodology for Semantic Utterance Classification in Virtual Human Dialogue Systems",
        "doc_id": "bc5c593b31d8f9614e44155a345d633e",
        "_score": 118.45227,
        "objective_sentences": "We present A Deep Learning Methodology for Semantic Utterance Classification in Virtual Human Dialogue Systems. In order to capture the candidate relationships between the semantic class and the word sequence in an utterance, we have proposed a shallow convolutional neural network (CNN) along with a recurrent neural network (RNN) that uses domain-specific word embeddings which have been initialized using Word2Vec for determining semantic similarity of words.",
        "sem_score": 2.071428571428571
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present Semantic Textual Similarity Methods, Tools, and Applications: A Survey. Measuring Semantic Textual Similarity (STS), between words/ terms, sentences, paragraph and document plays an important role in computer science and computational linguistic. It also has many application sover several fields such as Biomedical Informatics and Geoinformation. In this paper, we present a survey on different methods of textual similarity and we also reported about the availability of different software and tools those are useful for STS. In natural language processing (NLP), STS is a important component formany tasks such as document summarization, word sense disambiguation, short answer grading, information retrieval and extraction. We split out the measures for semantic similarity into three broad categories such as (i) Topological/Knowledge-based (ii) Statistical/Corpus Based (iii) String based. More emphasisi s given to the methods related to the WordNet taxonomy. Because topological methods, plays an important role to understand intended meaning of an ambiguous word, which is very difficult to process computationally. We also propose a new method for measuring semantic similarity between sentences. This proposed method, uses the advantages of taxonomy methods and merge these information to a language model. It considers the WordNet synsets for lexical relationships between nodes/words and a uni-gram language model is implemented over a large corpus to assign the information content value between the two nodes of different classes.",
        "title": "Semantic Textual Similarity Methods, Tools, and Applications: A Survey",
        "doc_id": "c8224c7b9253cb428ffd0610ce7b17ce",
        "_score": 127.421936,
        "objective_sentences": "We present Semantic Textual Similarity Methods, Tools, and Applications: A Survey. In this paper, we present a survey on different methods of textual similarity and we also reported about the availability of different software and tools those are useful for STS.",
        "sem_score": 2.0666666666666664
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present Semantic Textual Similarity Methods, Tools, and Applications: A Survey. Measuring Semantic Textual Similarity (STS), between words/ terms, sentences, paragraph and document plays an important role in computer science and computational linguistic. It also has many application sover several fields such as Biomedical Informatics and Geoinformation. In this paper, we present a survey on different methods of textual similarity and we also reported about the availability of different software and tools those are useful for STS. In natural language processing (NLP), STS is a important component formany tasks such as document summarization, word sense disambiguation, short answer grading, information retrieval and extraction. We split out the measures for semantic similarity into three broad categories such as (i) Topological/Knowledge-based (ii) Statistical/Corpus Based (iii) String based. More emphasisi s given to the methods related to the WordNet taxonomy. Because topological methods, plays an important role to understand intended meaning of an ambiguous word, which is very difficult to process computationally. We also propose a new method for measuring semantic similarity between sentences. This proposed method, uses the advantages of taxonomy methods and merge these information to a language model. It considers the WordNet synsets for lexical relationships between nodes/words and a uni-gram language model is implemented over a large corpus to assign the information content value between the two nodes of different classes.",
        "title": "Semantic Textual Similarity Methods, Tools, and Applications: A Survey",
        "doc_id": "c8224c7b9253cb428ffd0610ce7b17ce",
        "_score": 127.421936,
        "objective_sentences": "We present Semantic Textual Similarity Methods, Tools, and Applications: A Survey. In this paper, we present a survey on different methods of textual similarity and we also reported about the availability of different software and tools those are useful for STS.",
        "sem_score": 2.0666666666666664
    },
    {
        "date": "2014-01-01",
        "raw_text": "We present 16S ribosomal DNA sequence-based identification of bacteria in laboratory rodents: a practical approach in laboratory animal bacteriology diagnostics. Correct identification of bacteria is crucial for the management of rodent colonies. Some bacteria are difficult to identify phenotypically outside reference laboratories. In this study, we evaluated the utility of 16S ribosomal DNA (rDNA) sequencing as a means of identifying a collection of 30 isolates of rodent origin which are conventionally difficult to identify. Sequence analysis of the first approximate 720 to 880\u2009bp of the 5\u2032- end of 16S rDNA identified 25 isolates (83.33%) with \u226599% similarity to a sequence of a type strain, whereas three isolates (10%) displayed a sequence similarity \u226597% but <99% to the type strain sequences. These similarity scores were used to define identification to species and genus levels, respectively. Two of the 30 isolates (6.67%) displayed a sequence similarity of \u226595 but <97% to the reference strains and were thus allocated to a family. This technique allowed us to document the association of mice with bacteria relevant for the colonies management such as Pasteurellac...",
        "title": "16S ribosomal DNA sequence-based identification of bacteria in laboratory rodents: a practical approach in laboratory animal bacteriology diagnostics",
        "doc_id": "c8d4fe60514b2ddcb2377889f0ee0045",
        "_score": 127.28148,
        "objective_sentences": "We present 16S ribosomal DNA sequence-based identification of bacteria in laboratory rodents: a practical approach in laboratory animal bacteriology diagnostics. In this study, we evaluated the utility of 16S ribosomal DNA (rDNA) sequencing as a means of identifying a collection of 30 isolates of rodent origin which are conventionally difficult to identify.",
        "sem_score": 2.025
    },
    {
        "date": "2018-01-01",
        "raw_text": "We present Similarity Measures for the Detection of Clinical Conditions with Verbal Fluency Tasks. Semantic Verbal Fluency tests have been used in the detection of certain clinical conditions, like Dementia. In particular, given a sequence of semantically related words, a large number of switches from one semantic class to another has been linked to clinical conditions. In this work, we investigate three similarity measures for automatically identify switches in semantic chains: semantic similarity from a manually constructed resource, and word association strength and semantic relatedness, both calculated from corpora. This information is used for building classifiers to distinguish healthy controls from clinical cases with early stages of Alzheimers Disease and Mild Cognitive Deficits. The overall results indicate that for clinical conditions the classifiers that use these similarity measures outperform those that use a gold standard taxonomy.",
        "title": "Similarity Measures for the Detection of Clinical Conditions with Verbal Fluency Tasks",
        "doc_id": "550a06987a83b00d85e16e137239fb5d",
        "_score": 128.63943,
        "objective_sentences": "We present Similarity Measures for the Detection of Clinical Conditions with Verbal Fluency Tasks. In this work, we investigate three similarity measures for automatically identify switches in semantic chains: semantic similarity from a manually constructed resource, and word association strength and semantic relatedness, both calculated from corpora.",
        "sem_score": 2.0
    },
    {
        "date": "2017-01-01",
        "raw_text": "We present KeyphraseDS: Automatic generation of survey by exploiting keyphrase information. In this paper, we present a novel document summarization mechanism called KeyphraseDS that can organize the scientific articles into multi-aspect and informative scientific survey by exploiting keyphrases. Keyphrases describe text's salience and central focus, which can serve as the component of aspects under specific topic. KeyphraseDS consists of three steps: keyphrase graph construction, semantic aspect generation and content selection. Keyprhases are firstly extracted through CRF-based model exploiting various features, such as syntactic features, correlation features, etc. Spectral clustering is then performed on keyphrase graph to generate different aspects, where the semantic relatedness between keyphrases is computed through knowledge-based similarity and topic-based similarity. The proposed semantic relatedness can not only utilize the statistical text signals efficiently but also overcome the data sparsity problem. Significant sentences are then selected with respect to the generated aspects through integer linear programming (ILP), which takes semantic relevance, semantic diversity, and keyphrase salience into consideration. Extensive experiments, measured by automatic evaluation and human evaluation, demonstrate the effectiveness of our mechanism for generating scientific survey.",
        "title": "KeyphraseDS: Automatic generation of survey by exploiting keyphrase information",
        "doc_id": "a36e656ab97c4a4db096f768452028e5",
        "_score": 116.74211,
        "objective_sentences": "We present KeyphraseDS: Automatic generation of survey by exploiting keyphrase information. In this paper, we present a novel document summarization mechanism called KeyphraseDS that can organize the scientific articles into multi-aspect and informative scientific survey by exploiting keyphrases.",
        "sem_score": 2.0
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present Sentence Similarity Learning by Lexical Decomposition and Composition. Most conventional sentence similarity methods only focus on similar parts of two input sentences, and simply ignore the dissimilar parts, which usually give us some clues and semantic meanings about the sentences. In this work, we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences. The model represents each word as a vector, and calculates a semantic matching vector for each word based on all words in the other sentence. Then, each word vector is decomposed into a similar component and a dissimilar component based on the semantic matching vector. After this, a two-channel CNN model is employed to capture features by composing the similar and dissimilar components. Finally, a similarity score is estimated over the composed feature vectors. Experimental results show that our model gets the state-of-the-art performance on the answer sentence selection task, and achieves a comparable result on the paraphrase identification task.",
        "title": "Sentence Similarity Learning by Lexical Decomposition and Composition",
        "doc_id": "8bf2dc9c98cfb53c1e7dd9e58c34755b",
        "_score": 111.06212,
        "objective_sentences": "We present Sentence Similarity Learning by Lexical Decomposition and Composition. In this work, we propose a model to take into account both the similarities and dissimilarities by decomposing and composing lexical semantics over sentences.",
        "sem_score": 2.0
    },
    {
        "date": "2014-01-01",
        "raw_text": "We present PMI Based Clustering Algorithm forFeature Reduction in TextClassification. Feature clustering is a feature reduction method that reduces the dimensionality of feature vectors for text classification. In this paper an incremental feature clustering approach is proposed that uses Semantic similarity to cluster the features. Pointwise Mutual Information (PMI) is widely used word similarity measure, which finds Semantic similarity between two words and is an alternative for distributional similarity. PMI computation requires simple statistics about two words for similarity measure, that is number of cooccurrences or correlations between two concepts of fixed size are computed. Once the words from preprocessed documents are fed, clusters are formed and one feature (head word) is identified for each cluster which are used for indexing the document. PMI assumes that a word have single sense, but clustering can be optimized further if polysemies of words are considered. Hence PMI may be combined with PMImax, which estimates correlation between the closest senses of two words also, thereby better feature reduction and execution time compared with other approaches.",
        "title": "PMI Based Clustering Algorithm forFeature Reduction in TextClassification",
        "doc_id": "fcfd529d28a2df83fbc0313748f33416",
        "_score": 139.62964,
        "objective_sentences": "We present PMI Based Clustering Algorithm forFeature Reduction in TextClassification. In this paper an incremental feature clustering approach is proposed that uses Semantic similarity to cluster the features.",
        "sem_score": 2.0
    },
    {
        "date": "2015-01-01",
        "raw_text": "We present Similarity Technique used in IR and its Application. With large number of documents on the web, there is a increasing need to be able to retrieve the best relevant document. There are different techniques through which we can retrieve most relevant document from the large corpus. Similarity between words, sentences, paragraphs and documents is an important component in various tasks such as information retrieval, document clustering, word-sense disambiguation, automatic essay scoring, short answer grading, machine translation and text summarization. Text similarity means users query text is matched with the document text and on the basis on this matching user retrieves the most relevant documents. Text similarity also plays an important role in the categorization of text as well as document. We can measure the similarity between sentences, words, paragraphs and documents to categorize them in an efficient way. On the basis of this categorization, we can retrieve the best relevant document corresponding to users query. This paper describes different types of similarity like lexical similarity, semantic similarity etc. General Term Text Similarity, Text Mining, Text Summarization Keyword Text similarity, Lexical similarity, semantic similarity, Corpus based similarity and Knowledge based similarity.",
        "title": "Similarity Technique used in IR and its Application",
        "doc_id": "d5f994efc16a5910bc5ee011bdc0f111",
        "_score": 137.72,
        "objective_sentences": "We present Similarity Technique used in IR and its Application. There are different techniques through which we can retrieve most relevant document from the large corpus.",
        "sem_score": 2.0
    },
    {
        "date": "2015-01-01",
        "raw_text": "We present Calculation of Textual Similarity Using Semantic Relatedness Functions. Semantic similarity between two sentences is concerned with meas- uring how much two sentences share the same or related meaning. Two meth- ods in the literature for measuring sentence similarity are cosine similarity and overall similarity. In this work we investigate if it is possible to improve the performance of these methods by integrating different word level semantic relatedness methods. Four different word relatedness methods are compared using four different data sets compiled from different domains, providing a testbed formed of various range of writing expressions to challenge the selected methods. Results show that the use of corpus-based word semantic similarity function has significantly outperformed that of WordNet-based word semantic similarity function in sentence similarity methods. Moreover, we propose a new sentence similarity measure method by modifying an existing method which incorporates word order and lexical similarity called as overall similarity. Furthermore, the results show that the proposed method has significantly im- proved the performance of the overall method. All the selected methods are tested and compared with other state-of-the-art methods.",
        "title": "Calculation of Textual Similarity Using Semantic Relatedness Functions",
        "doc_id": "57e34a9b81aa32687cecc9fa54afa20c",
        "_score": 129.23584,
        "objective_sentences": "We present Calculation of Textual Similarity Using Semantic Relatedness Functions. In this work we investigate if it is possible to improve the performance of these methods by integrating different word level semantic relatedness methods.",
        "sem_score": 2.0
    },
    {
        "date": "2014-01-01",
        "raw_text": "We present Semi-supervised learning of dialogue acts using sentence similarity based on word embeddings. This paper describes a methodology for semi-supervised learning of dialogue acts using the similarity between sentences. We suppose that the dialogue sentences with the same dialogue act are more similar in terms of semantic and syntactic information. However, previous work on sentence similarity mainly modeled a sentence as bag-of-words and then compared different groups of words using corpus-based or knowledge-based measurements of word semantic similarity. Novelly, we present a vector-space sentence representation, composed of word embeddings, that is, the related word distributed representations, and these word embeddings are organised in a sentence syntactic structure. Given the vectors of the dialogue sentences, a distance measurement can be well-defined to compute the similarity between them. Finally, a seeded k-means clustering algorithm is implemented to classify the dialogue sentences into several categories corresponding to particular dialogue acts. This constitutes the semi-supervised nature of the approach, which aims to ameliorate the reliance of the availability of annotated corpora. Experiments with Switchboard Dialog Act corpus show that classification accuracy is improved by 14%, compared to the state-of-art methods based on Support Vector Machine.",
        "title": "Semi-supervised learning of dialogue acts using sentence similarity based on word embeddings",
        "doc_id": "6e960c2cf1e0e3fff2e99641c109e5bc",
        "_score": 124.95564,
        "objective_sentences": "We present Semi-supervised learning of dialogue acts using sentence similarity based on word embeddings. We suppose that the dialogue sentences with the same dialogue act are more similar in terms of semantic and syntactic information.",
        "sem_score": 2.0
    },
    {
        "date": "2014-01-01",
        "raw_text": "We present Semantic Search: Document Ranking and Clustering Using Computer Science Ontology and N-Grams. Semantic similarity has become an important tool and widely been used to solve traditional Information Retrieval problems. This study adopts ontology of computer science and proposes an ontology indexing weight based on Wu and Palmers edge counting measure and uses the N-grams method for computing a family of word similarity. The study also compares the subsumption weight between Hliaoutakis and Nicolas weight and query keywords (Decision Making, Genetic Algorithm, Machine Learning, Heuristic ). A probability value (p-values) from the t-test (p = 0.105) is higher 0.05, which indicates the evidence of no of no significant differences between the two weights methods. The experimental results show the new keyword-keyword similarity matrix scores that compute from hierarchical relationship weight based on Computer Science ontology and string matching (tri-grams) for computing of string of keyword. We computed the document-document similarity matrix scores using our keyword similarity matrix scores and compared them with the keyword matching weights using Dice coefficient method. In addition, this paper, we presented a new document semantic ranking process for the semantic ranking that proposes a new weight of query term in the document based on Computer Science Ontology weight. The experimental results show that the new document similarity score between a users query and the paper suggests that the new measures were effectively ranked.",
        "title": "Semantic Search: Document Ranking and Clustering Using Computer Science Ontology and N-Grams",
        "doc_id": "9623d049ab582d3a42180b3110594d0c",
        "_score": 134.20091,
        "objective_sentences": "We present Semantic Search: Document Ranking and Clustering Using Computer Science Ontology and N-Grams. We computed the document-document similarity matrix scores using our keyword similarity matrix scores and compared them with the keyword matching weights using Dice coefficient method.",
        "sem_score": 1.9381598793363497
    },
    {
        "date": "2012-01-01",
        "raw_text": "We present Unified Approach for Computing Document Similarity with Fingerprinting and Alignments. A fingerprinting algorithm and a sequence alignment are widely used for measuring the similarity of documents. The former algorithm is a very fast procedure that extracts and compares document's features. However, a fingerprinting algorithm cannot determine the partial similarity of a document. The latter algorithm is a procedure that arranges sequences of string to find similar regions. Sequence alignment is very effective in comparison to short strings but takes relatively long computing times. In this paper, we propose the MLA (Multi-Level Alignment) system, which combines a fingerprinting algorithm and a sequence alignment. The MLA system is designed for obtaining the advantages of both methods. This system uses a segmented block with uniform length as a basic operating unit. A similarity table of two input documents can be generated by comparing each document's blocks using a fingerprinting algorithm. Then, sequence alignment needs to be applied in the similarity table in order to identify similar regions. The proportion of the fingerprint algorithm and the sequence alignment in the MLA system is determined by the basic operation block's size $k$. If $k$ is 1 then the MLA system operates the same as sequence alignment. However, if $k$ is larger than the documents size, then it operates the same as a fingerprinting algorithm. Using this system, we prove that computing document's similarity with the hybrid-approach is faster than sequence alignment and also more accurate than the fingerprinting algorithm.",
        "title": "Unified Approach for Computing Document Similarity with Fingerprinting and Alignments",
        "doc_id": "d86ddb1241f83cd96e7c2d7036e8f0d0",
        "_score": 172.59317,
        "objective_sentences": "We present Unified Approach for Computing Document Similarity with Fingerprinting and Alignments. In this paper, we propose the MLA (Multi-Level Alignment) system, which combines a fingerprinting algorithm and a sequence alignment.",
        "sem_score": 1.9111111111111112
    },
    {
        "date": "2013-07-23",
        "raw_text": "We present SSM-DBSCANand SSM-OPTICS: Incorporating a new similarity measure for Density based Clustering of Web usage data.. Abstract: Clustering web sessions is to group web sessions based on similarity and consists of minimizing the intra-group similarity and maximizing the inter-group similarity. Here in this paper we developed a new similarity measure named SSM(Sequence Similarity Measure) and enhanced an existing DBSCAN and OPTICS clustering techniques namely SSM-DBSCAN, and SSM-OPTICS for clustering web sessions for web personalization. Then we adopted various similarity measures like Euclidean distance, Jaccard, Cosine and Fuzzy similarity measures to measure the similarity of web sessions using sequence alignment to determine learning behaviors of web usage data. This new measure has significant results when comparing similarities between web sessions with other previous measures. We performed a variety of experiments in the context of density based clustering, using existing DBSCAN and OPTICS and developed SSM-DBSCAN and SSM-OPTICS based on sequence alignment to measure similarities between web sessions where sessions are chronologically ordered sequences of page visits. Finally the time and the memory required to perform clustering using SSM is less when compared to other similarity measures.",
        "title": "SSM-DBSCANand SSM-OPTICS: Incorporating a new similarity measure for Density based Clustering of Web usage data.",
        "doc_id": "9e9af4410a743a5f7d00c1cb223630b5",
        "_score": 130.8038,
        "objective_sentences": "We present SSM-DBSCANand SSM-OPTICS: Incorporating a new similarity measure for Density based Clustering of Web usage data.. Here in this paper we developed a new similarity measure named SSM(Sequence Similarity Measure) and enhanced an existing DBSCAN and OPTICS clustering techniques namely SSM-DBSCAN, and SSM-OPTICS for clustering web sessions for web personalization.",
        "sem_score": 1.9047619047619047
    },
    {
        "date": "2011-01-01",
        "raw_text": "We present Query expansion based on a semantic graph model. Query expansion is a classical topic in the field of information retrieval, which is proposed to bridge the gap between searchers' information intents and their queries. Previous researches usually expand queries based on document collections, or some external resources such as WordNet and Wikipedia [1, 2, 3, 4, 5]. However, it seems that independently using one of these resources has some defects, document collections lack semantic information of words, while WordNet and Wikipedia may not include domain-specific knowledge in certain document collection. Our work aims to combine these two kinds of resources to establish an expansion model which represents not only domain-specific information but also semantic information. In our preliminary experiments, we construct a two-layer word graph and use Random-Walk algorithm to calculate the weights of each term in pseudo-relevance feedback documents, then select the highest weighted term to expand original query. The first layer of the word graph contains terms in related documents, while the second layer contains semantic senses corresponding to these terms. These terms and semantic senses are treated as vertices of the graph and connected with each other by all possible relationships, such as mutual information and semantic similarities. We utilized mutual information, semantic similarity and uniform distribution as the weight of term-term relation, sense-sense relation and word-sense relation respectively. Though these experiments show that our expansion outperform original queries, we are troubled with some difficult problems.   Given the framework of semantic graph model, we need more effort to find out an optimal graph to represent the relationships between terms and their semantic senses. We utilized a two-layer graph model in our preliminary research, where terms from different documents are treated equally. Maybe we can introduce the document as a third layer in future work, where we can differ the same terms in different documents according to document relevance and context.   Then we need appropriately represent initial weights of this words, senses and relationships. Various measures for weights of terms and term relations have been proved effective in other information retrieval tasks, such as TFIDF, mutual information (MI), but there is little research on weights for semantic senses and their relations. For polysemous words, we add all of their semantic senses to the graph and assume that these senses are uniformly distributed. Actually, it is not precise for a word in a special document and query. As we know, a polysemous word may have only one or two senses in a document, and they are not uniformly distributed. Give a word, what we should do is to determine its word senses in a relevant document and estimate the distribution of these senses. Word sense disambiguation may help us in this problem. Then, there are many methods to compute word similarity according to WordNet, which we use to represent the weights of relationships between word senses. Varelas et al implemented some popular methods to compute semantic similarity by mapping terms to an ontology and examining their relationships in that ontology [4]. We also need to know which algorithm for semantic similarity is most suitable for our model.   Additional, WordNet is suitable to calculate word similarity but not suitable to measure word relevance. The inner hyperlinks of Wikipedia could help us to calculate word relevance. We wish to find an effective way to combine the similarity measure from WordNet and relevance measure from Wikipedia, which may completely reflect word relationships.",
        "title": "Query expansion based on a semantic graph model",
        "doc_id": "090ce9a1a5743d4af7b876f3f977e8c4",
        "_score": 159.54698,
        "objective_sentences": "We present Query expansion based on a semantic graph model. In our preliminary experiments, we construct a two-layer word graph and use Random-Walk algorithm to calculate the weights of each term in pseudo-relevance feedback documents, then select the highest weighted term to expand original query.",
        "sem_score": 1.8
    },
    {
        "date": "2011-01-01",
        "raw_text": "We present Building a topic hierarchy using the bag-of-related-words representation. A simple and intuitive way to organize a huge document collection is by a topic hierarchy. Generally two steps are carried out to build a topic hierarchy automatically: 1) hierarchical document clustering and 2) cluster labeling. For both steps, a good textual document representation is essential. The bag-of-words is the common way to represent text collections. In this representation, each document is represented by a vector where each word in the document collection represents a dimension (feature). This approach has well known problems as the high dimensionality and sparsity of data. Besides, most of the concepts are composed by more than one word, as \"document engineering\" or \"text mining\". In this paper an approach called bag-of-related-words is proposed to generate features compounded by a set of related words with a dimensionality smaller than the bag-of-words. The features are extracted from each textual document of a collection using association rules. Different ways to map the document into transactions in order to allow the extraction of association rules and interest measures to prune the number of features are analyzed. To evaluate how much the proposed approach can aid the topic hierarchy building, we carried out an objective evaluation for the clustering structure, and a subjective evaluation for topic hierarchies. All the results were compared with the bag-of-words. The obtained results demonstrated that the proposed representation is better than the bag-of-words for the topic hierarchy building.",
        "title": "Building a topic hierarchy using the bag-of-related-words representation",
        "doc_id": "61425b1303c13f2050a5a20b7669c987",
        "_score": 137.26839,
        "objective_sentences": "We present Building a topic hierarchy using the bag-of-related-words representation. In this paper an approach called bag-of-related-words is proposed to generate features compounded by a set of related words with a dimensionality smaller than the bag-of-words.",
        "sem_score": 1.8
    },
    {
        "date": "2015-01-01",
        "raw_text": "We present An improved focused crawler based on Semantic Similarity Vector Space Model. An improved retrieval model - the Semantic Similarity Vector Space Model (SSVSM).The proposed model accurately predicts the unvisited URLs - priorities to the given topic.The proposed model guides focused crawlers to download large quantity and high quality web pages. A focused crawler is topic-specific and aims selectively to collect web pages that are relevant to a given topic from the Internet. In many studies, the Vector Space Model (VSM) and Semantic Similarity Retrieval Model (SSRM) take advantage of cosine similarity and semantic similarity to compute similarities between web pages and the given topic. However, if there are no common terms between a web page and the given topic, the VSM will not obtain the proper topical similarity of the web page. In addition, if all of the terms between them are synonyms, then the SSRM will also not obtain the proper topical similarity. To address these problems, this paper proposes an improved retrieval model, the Semantic Similarity Vector Space Model (SSVSM), which integrates the TF*IDF values of the terms and the semantic similarities among the terms to construct topic and document semantic vectors that are mapped to the same double-term set, and computes the cosine similarities between these semantic vectors as topic-relevant similarities of documents, including the full texts and anchor texts of unvisited hyperlinks. Next, the proposed model predicts the priorities of the unvisited hyperlinks by integrating the full text and anchor text topic-relevant similarities. The experimental results demonstrate that this approach improves the performance of the focused crawlers and outperforms other focused crawlers based on Breadth-First, VSM and SSRM. In conclusion, this method is significant and effective for focused crawlers.",
        "title": "An improved focused crawler based on Semantic Similarity Vector Space Model",
        "doc_id": "9bd97a989a0ba6534a1983e794d50eff",
        "_score": 133.11136,
        "objective_sentences": "We present An improved focused crawler based on Semantic Similarity Vector Space Model. To address these problems, this paper proposes an improved retrieval model, the Semantic Similarity Vector Space Model (SSVSM), which integrates the TF*IDF values of the terms and the semantic similarities among the terms to construct topic and document semantic vectors that are mapped to the same double-term set, and computes the cosine similarities between these semantic vectors as topic-relevant similarities of documents, including the full texts and anchor texts of unvisited hyperlinks.",
        "sem_score": 1.7888888888888892
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present Characterizing the Relationship Between Related and Unrelated Items in Recognition Memory. Previous work has shown that semantic similarity results in a memory bias in which related words are more likely than unrelated words to be labeled as studied in recognition memory. I explored the relationship between semantic similarity memory bias and memory for unrelated words. I varied the strength of the related word memory bias by manipulating the proportion of related to unrelated words, and the type of related word used. I showed that as the bias for related words increases, the unrelated false alarm rate decreases. To further characterize the relationship between related and unrelated words, I examined how the related and unrelated words affect memory decisions when they are experienced separately at test. This manipulation diminished the related word memory bias, but the decrease in unrelated word false alarms remained. These findings suggest a compelling relationship between semantic similarity and unrelated items that warrants further investigation. Characterizing the Relationship Between Related and Unrelated Items in Recognition Memory",
        "title": "Characterizing the Relationship Between Related and Unrelated Items in Recognition Memory",
        "doc_id": "bace9deb2addb726795a9006e8767fb5",
        "_score": 114.934845,
        "objective_sentences": "We present Characterizing the Relationship Between Related and Unrelated Items in Recognition Memory. I explored the relationship between semantic similarity memory bias and memory for unrelated words.",
        "sem_score": 1.7857142857142856
    },
    {
        "date": "2012-01-01",
        "raw_text": "We present Unsupervised similarity-based word sense disambiguation using context vectors and sentential word importance. The process of identifying the actual meanings of words in a given text fragment has a long history in the field of computational linguistics. Due to its importance in understanding the semantics of natural language, it is considered one of the most challenging problems facing this field. In this article we propose a new unsupervised similarity-based word sense disambiguation (WSD) algorithm that operates by computing the semantic similarity between glosses of the target word and a context vector. The sense of the target word is determined as that for which the similarity between gloss and context vector is greatest. Thus, whereas conventional unsupervised WSD methods are based on measuring pairwise similarity between words, our approach is based on measuring semantic similarity between sentences. This enables it to utilize a higher degree of semantic information, and is more consistent with the way that human beings disambiguate; that is, by considering the greater context in which the word appears. We also show how performance can be further improved by incorporating a preliminary step in which the relative importance of words within the original text fragment is estimated, thereby providing an ordering that can be used to determine the sequence in which words should be disambiguated. We provide empirical results that show that our method performs favorably against the state-of-the-art unsupervised word sense disambiguation methods, as evaluated on several benchmark datasets through different models of evaluation.",
        "title": "Unsupervised similarity-based word sense disambiguation using context vectors and sentential word importance",
        "doc_id": "7c8d892bd96fe2384dad8234d94493cf",
        "_score": 133.20575,
        "objective_sentences": "We present Unsupervised similarity-based word sense disambiguation using context vectors and sentential word importance. In this article we propose a new unsupervised similarity-based word sense disambiguation (WSD) algorithm that operates by computing the semantic similarity between glosses of the target word and a context vector.",
        "sem_score": 1.75
    },
    {
        "date": "2018-01-01",
        "raw_text": "We present Learning semantic similarity in a continuous space. We address the problem of learning semantic representation of questions to measure similarity between pairs as a continuous distance metric. Our work naturally extends Word Movers Distance (WMD) [1] by representing text documents as normal distributions instead of bags of embedded words. Our learned metric measures the dissimilarity between two questions as the minimum amount of distance the intent (hidden representation) of one question needs to \"travel\" to match the intent of another question. We first learn to repeat, reformulate questions to infer intents as normal distributions with a deep generative model [2] (variational auto encoder). Semantic similarity between pairs is then learned discriminatively as an optimal transport distance metric (Wasserstein 2) with our novel variational siamese framework. Among known models that can read sentences individually, our proposed framework achieves competitive results on Quora duplicate questions dataset. Our work sheds light on how deep generative models can approximate distributions (semantic representations) to effectively measure semantic similarity with meaningful distance metrics from Information Theory.",
        "title": "Learning semantic similarity in a continuous space",
        "doc_id": "dc9a37d13e4c2c46acd2c0a7b1ffd0ad",
        "_score": 132.75269,
        "objective_sentences": "We present Learning semantic similarity in a continuous space. We address the problem of learning semantic representation of questions to measure similarity between pairs as a continuous distance metric.",
        "sem_score": 1.7333333333333334
    },
    {
        "date": "2011-01-01",
        "raw_text": "We present Document clustering and topic discovery based on semantic similarity in scientific literature. Unlabeled document collections are becoming increasingly common and mining such databases becomes a major challenge. It is a major issue to retrieve relevant documents from the larger document collection. By clustering the text documents, the documents sharing similar topics are grouped together. Incorporating semantic features will improve the accuracy of document clustering methods. In order to determine at a sight whether the content of a cluster are of user interest or not, topic discovery methods are required to tag each clusters identifying distinct and representative topic of each cluster. Most of the existing topic discovery methods often assign labels to clusters based on the terms that the clustered documents contain. In this paper a modified semantic-based model is proposed where related terms are extracted as concepts for concept-based document clustering by bisecting k-means algorithm and topic detection method for discovering meaningful labels for the document clusters based on semantic similarity by Testor theory. The proposed method is compared to the Topic Detection by Clustering Keywords method using F-measure and purity as evaluation metrics. Experimental results prove that the proposed semantic-based model outperforms the existing work.",
        "title": "Document clustering and topic discovery based on semantic similarity in scientific literature",
        "doc_id": "8e3fbedaabce2254b1713d6f71b4da91",
        "_score": 141.24654,
        "objective_sentences": "We present Document clustering and topic discovery based on semantic similarity in scientific literature. In this paper a modified semantic-based model is proposed where related terms are extracted as concepts for concept-based document clustering by bisecting k-means algorithm and topic detection method for discovering meaningful labels for the document clusters based on semantic similarity by Testor theory.",
        "sem_score": 1.7210084033613446
    },
    {
        "date": "2017-01-01",
        "raw_text": "We present Design and Development of a Framework for an Automatic Answer Evaluation System Based on Similarity Measures. Abstract The assessment of answers is an important process that requires great effort from evaluators. This assessment process requires high concentration without any fluctuations in mood. This substantiates the need to automate answer script evaluation. Regarding text answer evaluation, sentence similarity measures have been widely used to compare student written answers with reference texts. In this paper, we propose an automated answer evaluation system that uses our proposed cosine-based sentence similarity measures to evaluate the answers. Cosine measures have proved to be effective in comparing between free text student answers and reference texts. Here we propose a set of novel cosine-based sentence similarity measures with varied approaches of creating document vector space. In addition to this, we propose a novel synset-based word similarity measure for computation of document vectors coupled with varied approaches for dimensionality-reduction for reducing vector space dimensions. Thus, we propose 21 cosine-based sentence similarity measures and measured their performance using MSR paraphrase corpus and Li\u2019s benchmark datasets. We also use these measures for automatic answer evaluation system and compare their performances using the Kaggle short answer and essay dataset. The performance of the system-generated scores is compared with the human scores using Pearson correlation. The results show that system and human scores have correlation between each other.",
        "title": "Design and Development of a Framework for an Automatic Answer Evaluation System Based on Similarity Measures",
        "doc_id": "82eb99926bb36e8ec3feed713709fa6c",
        "_score": 111.058426,
        "objective_sentences": "We present Design and Development of a Framework for an Automatic Answer Evaluation System Based on Similarity Measures. In this paper, we propose an automated answer evaluation system that uses our proposed cosine-based sentence similarity measures to evaluate the answers.",
        "sem_score": 1.6666666666666665
    },
    {
        "date": "2013-01-01",
        "raw_text": "We present A Fast Searching for Similar Text Using Genomic Read Mapping Method. The most important consideration when detecting plagiarism is precision. Thus, the precise determination of the similarity of two documents is critical for the authors of documents. However, the problem complexity is increased by considering precision alone. Typically, the semantic detection of plagiarism has very high complexity, so a syntactic method for detecting plagiarism is used widely. The two main syntactic methods are sequence alignment and fingerprinting. Sequence alignment has powerful characteristics such as very high precision, because it is based on character-by-character comparisons. However, naive sequence alignment has a high space complexity (O(n 2 )). Fingerprinting is another syntactic method that uses the similarity of vectors extracted from documents. This method has a lower space complexity (O(n)) compared with sequence alignment. However, it also has lower precision because this method does not consider the structural similarity of documents. The method we propose for detecting plagiarized texts can detect plagiarism precisely, even with a low spatiotemporal complexity, by applying the short-read mapping method used for next-generation sequencing (NGS). In addition, we propose a distance measure for documents, which is based on the detection method used to construct phylogenetic tree by calculating the similarities of documents. The proposed method has a maximum precision of 0.95 and a maximum recall of 0.94. The construction of phylogenetic trees for linearly plagiarized documents using the distance measure had an average precision of 0.99. In the future, we will study the phylogeny of naturally plagiarized documents.",
        "title": "A Fast Searching for Similar Text Using Genomic Read Mapping Method",
        "doc_id": "002c328e2facc5d119f152d1d0b6948e",
        "_score": 139.97144,
        "objective_sentences": "We present A Fast Searching for Similar Text Using Genomic Read Mapping Method. The method we propose for detecting plagiarized texts can detect plagiarism precisely, even with a low spatiotemporal complexity, by applying the short-read mapping method used for next-generation sequencing (NGS).",
        "sem_score": 1.6666666666666665
    },
    {
        "date": "2015-01-01",
        "raw_text": "We present Learning Distributed Document Representations for Multi-label Document Categorization. Multi-label Document Categorization, the task of automatically assigning a text document into one or more categories has various real-world applications such as categorizing news articles, tagging Web pages, maintaining medical patient records and organizing digital libraries among many others. Statistical Machine Learning approaches to document categorization have focused on multi-label learning algorithms such as Support Vector Machines, k-Nearest Neighbors, Logistic Regression, Neural Networks, Naive Bayes, Generative Probabilistic Models etc. while the input to such algorithms i.e. the vector representation for documents has traditionally been used as the bag-of-words model. Though the usage of simple bag-of-words representation gives surprisingly accurate results, it suffers from sparsity, high-dimensionality, lack of similarity measures along with other drawbacks such as the inability to encode word ordering and contextual information in which the words occur. Encoding contextual information about words in documents is crucial to capture the correct semantic content of the highly complex and ambiguous human language. Our work is focused on learning continuous distributed vector representations for documents by embedding all the documents in the same low-dimensional space such that documents that are similar in their semantic content have similar vector representations. To tackle the issues in bag-of-words representation model, we present an unsupervised neural network model that uses the document vector to predict words in the document along with using the contextual information in which the word occurs and jointly learns distributed document and word representations. We develop a modified version of the logistic regression algorithm to learn similar distributed representations for categories to perform the document categorization task. We show that our model gives state-of-the-art results on the standard Reuters-21578 dataset, improving the bag-of-words model by 9% and previous state-of-the-art by 3.26% in terms of the F1 Score. We also show the effectiveness of our model in imputing missing categories on the Wikipedia articles against the bag-of-words representations. As we embed documents, categories and words in the same low-dimensional space our model can also estimate semantic similarities between them. We qualitatively demonstrate that the learned representations capture the semantic dependencies between categories and words which is not directly observed in the data.",
        "title": "Learning Distributed Document Representations for Multi-label Document Categorization",
        "doc_id": "f0de1d185f0242943a4f1d277ac9652e",
        "_score": 135.57474,
        "objective_sentences": "We present Learning Distributed Document Representations for Multi-label Document Categorization. To tackle the issues in bag-of-words representation model, we present an unsupervised neural network model that uses the document vector to predict words in the document along with using the contextual information in which the word occurs and jointly learns distributed document and word representations.",
        "sem_score": 1.6666666666666665
    },
    {
        "date": "2012-01-01",
        "raw_text": "We present A Topic Similarity Model for Hierarchical Phrase-based Translation. Previous work using topic model for statistical machine translation (SMT) explore topic information at the word level. However, SMT has been advanced from word-based paradigm to phrase/rule-based paradigm. We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation. We associate each synchronous rule with a topic distribution, and select desirable rules according to the similarity of their topic distributions with given documents. We show that our model significantly improves the translation performance over the baseline on NIST Chinese-to-English translation experiments. Our model also achieves a better performance and a faster speed than previous approaches that work at the word level.",
        "title": "A Topic Similarity Model for Hierarchical Phrase-based Translation",
        "doc_id": "b2cb84aa1203ab7ec991f1d8bac635f8",
        "_score": 125.78847,
        "objective_sentences": "We present A Topic Similarity Model for Hierarchical Phrase-based Translation. We therefore propose a topic similarity model to exploit topic information at the synchronous rule level for hierarchical phrase-based translation.",
        "sem_score": 1.6666666666666665
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present Automatic Measurement of Semantic Similarity among Arabic Short Texts. Documents that are dealing with the same topic include normally many identical words. Accordingly, surface words co-occurrence similarity measures has been applied successfully to measure the similarity between these documents. However, the problem is not a trivial task when dealing with short texts that carry the same or close meaning but with different vocabularies. Toward solving this problem, researchers have been investigating methods for word analysis at the semantic level. We introduce a new method to measure the semantic similarity between short texts. In the proposed method, semantic distribution and lexical similarity measures are combined to determine the degree of similarity between two words. The similarity between two words is measured as the lexical similarity between the vectors of similar words extracted from corpus as a second order word vector. The proposed method was applied to measure the semantic similarity between Arabic short texts. The experiments performed showed that the best accuracy achieved by the proposed method was 97% compared to 93% recorded for the second order distribution similarity.",
        "title": "Automatic Measurement of Semantic Similarity among Arabic Short Texts",
        "doc_id": "45220aeba972203936c3c9b2102fac04",
        "_score": 128.81522,
        "objective_sentences": "We present Automatic Measurement of Semantic Similarity among Arabic Short Texts. We introduce a new method to measure the semantic similarity between short texts.",
        "sem_score": 1.6
    },
    {
        "date": "2014-01-29",
        "raw_text": "We present Sources of semantic similarity. Similarity is the key notion underlying many contemporary theories about the representation of meaning through words or concepts. However, these representations are strongly colored by the kind of information captured by various semantic measures. In this paper we present a systematic comparison of human similarity judgments and calculated similarity coefficients from different sources of semantic similarity based on concept features, word associations, word co-occurrence and expert knowledge. We show that these measures capture our semantic representations to a large extent, but also model different aspects of our semantic knowledge, depending on (a) the semantic domain and (b) the range of similarity comparisons.",
        "title": "Sources of semantic similarity",
        "doc_id": "5abd581624301f615b72d0c422f51b67",
        "_score": 128.72124,
        "objective_sentences": "We present Sources of semantic similarity. In this paper we present a systematic comparison of human similarity judgments and calculated similarity coefficients from different sources of semantic similarity based on concept features, word associations, word co-occurrence and expert knowledge.",
        "sem_score": 1.6
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present Automatic Measurement of Semantic Similarity among Arabic Short Texts. Documents that are dealing with the same topic include normally many identical words. Accordingly, surface words co-occurrence similarity measures has been applied successfully to measure the similarity between these documents. However, the problem is not a trivial task when dealing with short texts that carry the same or close meaning but with different vocabularies. Toward solving this problem, researchers have been investigating methods for word analysis at the semantic level. We introduce a new method to measure the semantic similarity between short texts. In the proposed method, semantic distribution and lexical similarity measures are combined to determine the degree of similarity between two words. The similarity between two words is measured as the lexical similarity between the vectors of similar words extracted from corpus as a second order word vector. The proposed method was applied to measure the semantic similarity between Arabic short texts. The experiments performed showed that the best accuracy achieved by the proposed method was 97% compared to 93% recorded for the second order distribution similarity.",
        "title": "Automatic Measurement of Semantic Similarity among Arabic Short Texts",
        "doc_id": "45220aeba972203936c3c9b2102fac04",
        "_score": 128.2309,
        "objective_sentences": "We present Automatic Measurement of Semantic Similarity among Arabic Short Texts. We introduce a new method to measure the semantic similarity between short texts.",
        "sem_score": 1.6
    },
    {
        "date": "2013-07-22",
        "raw_text": "We present Measuring Of Semantic Similarity Between Words Using WebSearch Engine Approach. Semantic similarity is the process of identifying the synonyms for a given word. Which returns the one or more words which give the same meaning in context. In dictionary the semantic similarity between words is solved. But when it comes to web, measuring the semantic similarity between words has become the challenging task.Inorder to find the semantic similarity between the words we have proposed a lexical pattern extraction algorithm to find the numerous semantic relations between two words. And also a sequential pattern clustering algorithm was proposed to find the number of lexical patterns that shows the same semantic relations between two words. Page count concurrence measures along with lexical patterns extracted from snippets are used to define features of a word pair. Testing on three benchmark desk by training two class SVM the proposed method outperformed various baselines. And also it also improved the efficiency of community mining. Pattern matching is the concept which reveals/deals with the similarity between words. It is useful in finding the files in a folder or disk Given text in a document etc. The concept of tries helps to achieve",
        "title": "Measuring Of Semantic Similarity Between Words Using WebSearch Engine Approach",
        "doc_id": "076f28796c9cbc831bae8e96b8609def",
        "_score": 124.540695,
        "objective_sentences": "We present Measuring Of Semantic Similarity Between Words Using WebSearch Engine Approach. Inorder to find the semantic similarity between the words we have proposed a lexical pattern extraction algorithm to find the numerous semantic relations between two words.",
        "sem_score": 1.6
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present Morphological Priors for Probabilistic Neural Word Embeddings. Word embeddings allow natural language processing systems to share statistical information across related words. These embeddings are typically based on distributional statistics, making it difficult for them to generalize to rare or unseen words. We propose to improve word embeddings by incorporating morphological information, capturing shared sub-word features. Unlike previous work that constructs word embeddings directly from morphemes, we combine morphological and distributional information in a unified probabilistic framework, in which the word embedding is a latent variable. The morphological information provides a prior distribution on the latent word embeddings, which in turn condition a likelihood function over an observed corpus. This approach yields improvements on intrinsic word similarity evaluations, and also in the downstream task of part-of-speech tagging.",
        "title": "Morphological Priors for Probabilistic Neural Word Embeddings",
        "doc_id": "0f9e60d1b6cab1670292e2b95a9cd745",
        "_score": 117.65737,
        "objective_sentences": "We present Morphological Priors for Probabilistic Neural Word Embeddings. We propose to improve word embeddings by incorporating morphological information, capturing shared sub-word features.",
        "sem_score": 1.5714285714285714
    },
    {
        "date": "2015-01-01",
        "raw_text": "We present - 4 ) A Web Search Engine-Based Approach to Measure Semantic Similarity between Words. easuring the semantic similarity between words is an important component in various tasks on the web such as relation extraction, community mining, document clustering, and automatic metadata extraction. Despite the usefulness of semantic similarity measures in these applications, accurately measuring semantic similarity between two words (or entities) remains a challenging task. We propose an empirical method to estimate semantic similarity using page counts and text snippets retrieved from a web search engine for two words. Specifically, we define various word co-occurrence measures using page counts and integrate those with lexical patterns extracted from text snippets. To identify the numerous semantic relations that exist between two given words, we propose a novel pattern extraction algorithm and a pattern clustering algorithm. The optimal combination of page counts-based co-occurrence measures and lexical pattern clusters is learned using support vector machines. The proposed method outperforms various baselines and previously proposed web-based semantic similarity measures on three benchmark data sets showing a high correlation with human ratings. Moreover, the proposed method significantly improves the accuracy in a community mining task.",
        "title": "- 4 ) A Web Search Engine-Based Approach to Measure Semantic Similarity between Words",
        "doc_id": "7db3b3787da239bd3cb7b485cca2bb01",
        "_score": 126.74712,
        "objective_sentences": "We propose an empirical method to estimate semantic similarity using page counts and text snippets retrieved from a web search engine for two words. Specifically, we define various word co-occurrence measures using page counts and integrate those with lexical patterns extracted from text snippets.",
        "sem_score": 1.5714285714285714
    },
    {
        "date": "2015-03-12",
        "raw_text": "We present Topic-based multi-document summarization with probabilistic la tent semantic analysis. We consider the problem of query-focused multi-document summarization, where a summary containing the information most relevant to a users information need is produced from a set of topic-related documents. We propose a new method based on probabilistic latent semantic analysis, which allows us to represent sentences and queries as probability distributions over la-tent topics. Our approach combines query-focused and thematic features computed in the latent topic space to estimate the summary-relevance of sentences. In addition, we evaluate several different similarity measures for comput-ing sentence-level feature scores. Experimental results show that our approach outperforms the best reported results on DUC 2006 data, and also compares well on DUC 2007 data.",
        "title": "Topic-based multi-document summarization with probabilistic la tent semantic analysis",
        "doc_id": "6fe556809f71361e8074606c3b2bd2d2",
        "_score": 126.3449,
        "objective_sentences": "We present Topic-based multi-document summarization with probabilistic la tent semantic analysis. We consider the problem of query-focused multi-document summarization, where a summary containing the information most relevant to a users information need is produced from a set of topic-related documents.",
        "sem_score": 1.5714285714285714
    },
    {
        "date": "2012-01-01",
        "raw_text": "We present Semantic Clustering of Genomic Documents Using Go Terms as Feature Set. The biological databases generate huge volume of genomics and proteomics data. The sequence information is used by researches to find similarity of genes, proteins and to find other related information. The genomic sequence database consists of large number of attributes as annotations, represented for defining the sequences in Xml format. It is necessary to have proper mechanism to group the documents for information retrieval. Data mining techniques like clustering and classification methods can be used to group the documents. The objective of the paper is to analyze the set of keywords which can be represented as features for grouping the documents semantically. This paper focuses on clustering genomic documents based on both structural and content similarity .The structural similarity is found using structural path between the documents. The semantic similarity is found for the structurally similar documents. We have proposed a methodology to cluster the genomic documents using sequence attributes without using the sequence data. The sequence attributes for genomic documents are analyzed using Filter based feature selection methods to find the relevant feature set for grouping the similar documents. Based on the attribute ranking we have clustered the similar documents using All Keyword approach (KBA) and GO Terms based approach (GOTA). The experimental results of the clusters are validated for two approaches by inferring biological meaning using Gene Ontology. From the results it was inferred that all keywords based approach grouped documents based on the semantic meaning of Gene Ontology terms. The GO terms based approach grouped larger number of documents without considering any other keywords, which is semantically relevant which results in reducing the complexity of the attributes considered. We claim that using GO terms can alone be used as features set to group genomic documents with high similarity.",
        "title": "Semantic Clustering of Genomic Documents Using Go Terms as Feature Set",
        "doc_id": "952f9a64462ac59812b7d6461e181c7b",
        "_score": 125.34977,
        "objective_sentences": "We present Semantic Clustering of Genomic Documents Using Go Terms as Feature Set. The objective of the paper is to analyze the set of keywords which can be represented as features for grouping the documents semantically.",
        "sem_score": 1.5714285714285714
    },
    {
        "date": "2017-01-01",
        "raw_text": "We present Enhanced word embedding similarity measures using fuzzy rules for query expansion. Query expansion has been widely used to select additional words that are related to the original query words in the field of information retrieval. In this paper, we present a novel query expansion method that jointly uses fuzzy rules and a word embedding similarity calculation. The expansion words are generated using a word embedding method and selected according to their semantic similarity to the original query. Fuzzy rules are used to enhance the word similarity calculations and reweight expansion words. When measuring and ranking the relevance of a retrieved document, the original query and the expansion words with their weights are considered. We conduct experiments on the query expansion in document ranking tasks. Experimental results from the document ranking task show that the proposed method is able to significantly outperform state-of-the-art baseline methods.",
        "title": "Enhanced word embedding similarity measures using fuzzy rules for query expansion",
        "doc_id": "b544947f9a7cb964792d9453e4b4378b",
        "_score": 142.21579,
        "objective_sentences": "We present Enhanced word embedding similarity measures using fuzzy rules for query expansion. In this paper, we present a novel query expansion method that jointly uses fuzzy rules and a word embedding similarity calculation.",
        "sem_score": 1.5555555555555556
    },
    {
        "date": "2017-01-01",
        "raw_text": "We present Automated question answering system using ontology and semantic role. Semantic similarity is an essential part for question answering, it is used various fields such as Artificial Intelligence, Natural Language Processing, information retrieval, Document Retrieval and Automatic evaluations. This paper mainly focuses on similarity measure based on the posted query, and finding the appropriate meaning between the words. Accessing an accurate answer from the web document is challenging task. The proposed approach is used to analyze and measuring the similarity between the words. It presents the Web And semantic knowledge-Driven automatic question answering system (WAD). It encompasses three phases to enhance the performance of QA system using the web as well as the semantic knowledge. Initially, the WAD approach determines the user query, query expansion technique and entity linking method. The ontology based information is used in WAD to rank the answers and experimental results provide the result with high accuracy than the baseline method.",
        "title": "Automated question answering system using ontology and semantic role",
        "doc_id": "258cc0fd7973def42234f3db5ffdfd2a",
        "_score": 113.29393,
        "objective_sentences": "We present Automated question answering system using ontology and semantic role. This paper mainly focuses on similarity measure based on the posted query, and finding the appropriate meaning between the words.",
        "sem_score": 1.5555555555555556
    },
    {
        "date": "2017-12-25",
        "raw_text": "We present Measuring similarity of academic articles with semantic profile and joint word embedding. Long-document semantic measurement has great significance in many applications such as semantic searchs, plagiarism detection, and automatic technical surveys. However, research efforts have mainly focused on the semantic similarity of short texts. Document-level semantic measurement remains an open issue due to problems such as the omission of background knowledge and topic transition. In this paper, we propose a novel semantic matching method for long documents in the academic domain. To accurately represent the general meaning of an academic article, we construct a semantic profile in which key semantic elements such as the research purpose, methodology, and domain are included and enriched. As such, we can obtain the overall semantic similarity of two papers by computing the distance between their profiles. The distances between the concepts of two different semantic profiles are measured by word vectors. To improve the semantic representation quality of word vectors, we propose a joint word-embedding model for incorporating a domain-specific semantic relation constraint into the traditional context constraint. Our experimental results demonstrate that, in the measurement of document semantic similarity, our approach achieves substantial improvement over state-of-the-art methods, and our joint word-embedding model produces significantly better word representations than traditional word-embedding models.",
        "title": "Measuring similarity of academic articles with semantic profile and joint word embedding",
        "doc_id": "c47c5c1fdd9baf2db890197872a51fe9",
        "_score": 159.28755,
        "objective_sentences": "We present Measuring similarity of academic articles with semantic profile and joint word embedding. In this paper, we propose a novel semantic matching method for long documents in the academic domain.",
        "sem_score": 1.5
    },
    {
        "date": "2017-01-01",
        "raw_text": "We present Bridging the gap: Incorporating a semantic similarity measure for effectively mapping PubMed queries to documents. The main approach of traditional information retrieval (IR) is to examine how many words from a query appear in a document. A drawback of this approach, however, is that it may fail to detect relevant documents where no or only few words from a query are found. The semantic analysis methods such as LSA (latent semantic analysis) and LDA (latent Dirichlet allocation) have been proposed to address the issue, but their performance is not superior compared to common IR approaches. Here we present a query-document similarity measure motivated by the Word Mover's Distance. Unlike other similarity measures, the proposed method relies on neural word embeddings to compute the distance between words. This process helps identify related words when no direct matches are found between a query and a document. Our method is efficient and straightforward to implement. The experimental results on TREC Genomics data show that our approach outperforms the BM25 ranking function by an average of 12% in mean average precision. Furthermore, for a real-world dataset collected from the PubMed search logs, we combine the semantic measure with BM25 using a learning to rank method, which leads to improved ranking scores by up to 25%. This experiment demonstrates that the proposed approach and BM25 nicely complement each other and together produce superior performance.",
        "title": "Bridging the gap: Incorporating a semantic similarity measure for effectively mapping PubMed queries to documents",
        "doc_id": "f0247ff415406c6eb831cca860165fdc",
        "_score": 135.43277,
        "objective_sentences": "We present Bridging the gap: Incorporating a semantic similarity measure for effectively mapping PubMed queries to documents. Here we present a query-document similarity measure motivated by the Word Mover's Distance.",
        "sem_score": 1.5
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present Efficient Graph-Based Document Similarity. Assessing the relatedness of documents is at the core of many applications such as document retrieval and recommendation. Most similarity approaches operate on word-distribution-based document representations - fast to compute, but problematic when documents differ in language, vocabulary or type, and neglecting the rich relational knowledge available in Knowledge Graphs. In contrast, graph-based document models can leverage valuable knowledge about relations between entities - however, due to expensive graph operations, similarity assessments tend to become infeasible in many applications. This paper presents an efficient semantic similarity approach exploiting explicit hierarchical and transversal relations. We show in our experiments that i our similarity measure provides a significantly higher correlation with human notions of document similarity than comparable measures, ii this also holds for short documents with few annotations, iii document similarity can be calculated efficiently compared to other graph-traversal based approaches.",
        "title": "Efficient Graph-Based Document Similarity",
        "doc_id": "fe00d9a497059342f8cb8da698ba3d37",
        "_score": 132.61931,
        "objective_sentences": "We present Efficient Graph-Based Document Similarity. This paper presents an efficient semantic similarity approach exploiting explicit hierarchical and transversal relations.",
        "sem_score": 1.5
    },
    {
        "date": "2018-01-01",
        "raw_text": "We present Topic based Summarization of Multiple Documents using Semantic Analysis and Clustering. Document summarization addresses the problem of presenting the information in a compact form to the readers. Different approaches to summarize documents have been proposed and evaluated in literature. Common research problems in multi-document summarization are Redundancy and Extraction of sentences; that are important and semantically linked with other sentences. With the combination of agglomerative hierarchical clustering and Latent Semantic Analysis (LSA); which measures semantic similarity between sentences and reduces dimensions by preserving only highly weighted vectors, we propose a novel multi document summarization approach. Latent Dirichlet Allocation Model is used to identify important topic terms in the resultant summary. We have used Recall Oriented Understudy for Gisting Evaluation (ROUGE) metric to evaluate our system against other state-of-the art techniques using Document Understanding Conference (DUC) dataset 2004. Experimental results show that there is substantial performance improvement using our system and it makes better summary as compared to the other state-of-art techniques.",
        "title": "Topic based Summarization of Multiple Documents using Semantic Analysis and Clustering",
        "doc_id": "b292e24b59905123e1b78b3e601845ec",
        "_score": 126.22962,
        "objective_sentences": "We present Topic based Summarization of Multiple Documents using Semantic Analysis and Clustering. With the combination of agglomerative hierarchical clustering and Latent Semantic Analysis (LSA); which measures semantic similarity between sentences and reduces dimensions by preserving only highly weighted vectors, we propose a novel multi document summarization approach.",
        "sem_score": 1.5
    },
    {
        "date": "2018-01-01",
        "raw_text": "We present An Ontology Driven Knowledge Block Summarization Approach for Chinese Judgment Document Classification. Efficient document classification techniques are crucial to current legal applications, such as case-based reasoning, legal citations, and so on. However, Chinese judgment documents are large and highly complex, so the traditional machine leaning-based classification models are often inefficient to Chinese document classification due to the fact that they fail to incorporate the overall structure and extra domain specific knowledge. In this paper, we propose an ontology-driven knowledge block summarization approach to computing document similarity for Chinese judgment document classification. First, the extra semantic knowledge for Chinese judgment documents is adopted from the perspectives of the top-level ontology and domain-specific ontologies, where how to merge the different kinds of ontologies together in an extensible manner is further represented. Second, the core semantic knowledge residing in Chinese judgment documents can be summarized into knowledge blocks by ontology-based information extraction. Third, we use Word Movers Distance (WMD) is to calculate the similarity between different knowledge blocks instead of their original Chinese judgment documents. At last, the KNN-based experiments for Chinese judgment document classification were made to illustrate that our approach is more effective in achieving higher classification accuracy and has faster computation speed compared to the original WMD approach.",
        "title": "An Ontology Driven Knowledge Block Summarization Approach for Chinese Judgment Document Classification",
        "doc_id": "fe86618938db27106004d53085c1d6fb",
        "_score": 112.61711,
        "objective_sentences": "We present An Ontology Driven Knowledge Block Summarization Approach for Chinese Judgment Document Classification. In this paper, we propose an ontology-driven knowledge block summarization approach to computing document similarity for Chinese judgment document classification.",
        "sem_score": 1.5
    },
    {
        "raw_text": "We present Finding an Optimal Corneal Xenograft Using Comparative Analysis of Corneal Matrix Proteins Across Species. Numerous animal species have been proposed as sources of corneal tissue for obtaining decellularized xenografts. The selection of an appropriate animal model must take into consideration the differences in the composition and structure of corneal proteins between humans and other animal species in order to minimize immune response and improve outcome of the xenotransplant. Here, we compared the amino-acid sequences of 16 proteins present in the corneal stromal matrix of 14 different animal species using Basic Local Alignment Search Tool, and calculated a similarity score compared to the respective human sequence. Primary amino acid structures, isoelectric point and grand average of hydropathy (GRAVY) values of the 7 most abundant proteins ( i.e . collagen \u03b1-1 (I), \u03b1-1 (VI), \u03b1-2 (I) and \u03b1-3 (VI), as well as decorin, lumican, and keratocan) were also extracted and compared to those of human. The pig had the highest similarity score (91.8%). All species showed a lower proline content compared to human. Isoelectric point of pig (7.1) was the closest to the human. Most species have higher GRAVY values compared to human except horse. Our results suggest that porcine cornea has a higher relative suitability for corneal transplantation into humans compared to other studied species.",
        "date": "2019-02-12",
        "title": "Finding an Optimal Corneal Xenograft Using Comparative Analysis of Corneal Matrix Proteins Across Species",
        "doc_id": "9eb7ed535dd0ca69f7894c9e3032e247",
        "_score": 110.31826,
        "objective_sentences": "We present Finding an Optimal Corneal Xenograft Using Comparative Analysis of Corneal Matrix Proteins Across Species. Here, we compared the amino-acid sequences of 16 proteins present in the corneal stromal matrix of 14 different animal species using Basic Local Alignment Search Tool, and calculated a similarity score compared to the respective human sequence.",
        "sem_score": 1.5
    },
    {
        "date": "2011-01-01",
        "raw_text": "We present Clustering More than Two Million Biomedical Publications: Comparing the Accuracies of Nine Text-Based Similarity Approaches. BackgroundWe investigate the accuracy of different similarity approaches for clustering over two million biomedical documents. Clustering large sets of text documents is important for a variety of information needs and applications such as collection management and navigation, summary and analysis. The few comparisons of clustering results from different similarity approaches have focused on small literature sets and have given conflicting results. Our study was designed to seek a robust answer to the question of which similarity approach would generate the most coherent clusters of a biomedical literature set of over two million documents.MethodologyWe used a corpus of 2.15 million recent (2004-2008) records from MEDLINE, and generated nine different document-document similarity matrices from information extracted from their bibliographic records, including titles, abstracts and subject headings. The nine approaches were comprised of five different analytical techniques with two data sources. The five analytical techniques are cosine similarity using term frequency-inverse document frequency vectors (tf-idf cosine), latent semantic analysis (LSA), topic modeling, and two Poisson-based language models \u2013 BM25 and PMRA (PubMed Related Articles). The two data sources were a) MeSH subject headings, and b) words from titles and abstracts. Each similarity matrix was filtered to keep the top-n highest similarities per document and then clustered using a combination of graph layout and average-link clustering. Cluster results from the nine similarity approaches were compared using (1) within-cluster textual coherence based on the Jensen-Shannon divergence, and (2) two concentration measures based on grant-to-article linkages indexed in MEDLINE.ConclusionsPubMed's own related article approach (PMRA) generated the most coherent and most concentrated cluster solution of the nine text-based similarity approaches tested, followed closely by the BM25 approach using titles and abstracts. Approaches using only MeSH subject headings were not competitive with those based on titles and abstracts.",
        "title": "Clustering More than Two Million Biomedical Publications: Comparing the Accuracies of Nine Text-Based Similarity Approaches",
        "doc_id": "13300ad6649d91a1769b201d7a8affff",
        "_score": 142.59744,
        "objective_sentences": "We present Clustering More than Two Million Biomedical Publications: Comparing the Accuracies of Nine Text-Based Similarity Approaches.",
        "sem_score": 1.5
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present Efficient Graph-Based Document Similarity. Assessing the relatedness of documents is at the core of many applications such as document retrieval and recommendation. Most similarity approaches operate on word-distribution-based document representations - fast to compute, but problematic when documents differ in language, vocabulary or type, and neglecting the rich relational knowledge available in Knowledge Graphs. In contrast, graph-based document models can leverage valuable knowledge about relations between entities - however, due to expensive graph operations, similarity assessments tend to become infeasible in many applications. This paper presents an efficient semantic similarity approach exploiting explicit hierarchical and transversal relations. We show in our experiments that i our similarity measure provides a significantly higher correlation with human notions of document similarity than comparable measures, ii this also holds for short documents with few annotations, iii document similarity can be calculated efficiently compared to other graph-traversal based approaches.",
        "title": "Efficient Graph-Based Document Similarity",
        "doc_id": "fe00d9a497059342f8cb8da698ba3d37",
        "_score": 132.61931,
        "objective_sentences": "We present Efficient Graph-Based Document Similarity. This paper presents an efficient semantic similarity approach exploiting explicit hierarchical and transversal relations.",
        "sem_score": 1.5
    },
    {
        "date": "2011-01-01",
        "raw_text": "We present CLIQUE: Clustering based on density on web usage data: Experiments and test results. Clustering web sessions is to group web sessions based on similarity and consists of minimizing the intra-group similarity and maximizing the inter-group similarity. The other question that arises is how to measure similarity between web sessions. Here in this paper we adopted a CLIQUE (CLUstering in QUEst) algorithm for clustering web sessions for web personalization. Then we adopted various similarity measures like Euclidean distance, projected Euclidean distance Jaccard, cosine and fuzzy dissimilarity measures to measure the similarity of web sessions using sequence alignment to determine learning behaviors. This has significant results when comparing similarities between web sessions with various measures, we performed a variety of experiments in the context of density based clustering, based on sequence alignment to measure similarities between web sessions where sessions are chronologically ordered sequences of page visits.",
        "title": "CLIQUE: Clustering based on density on web usage data: Experiments and test results",
        "doc_id": "2679494b53c73c570cbaf61c1aa154a8",
        "_score": 130.96455,
        "objective_sentences": "We present CLIQUE: Clustering based on density on web usage data: Experiments and test results. Here in this paper we adopted a CLIQUE (CLUstering in QUEst) algorithm for clustering web sessions for web personalization.",
        "sem_score": 1.5
    },
    {
        "date": "2013-12-23",
        "raw_text": "We present Multiple Parts-of Speech. Abstract. Semantic similarity aims at establishing resemblance by interpreting the meaning of the objects being compared. The Semantic Web can benefit from semantic similarity in several ways: ontology alignment and merging, automatic ontology construction, semantic-search, to cite a few. Current approaches mostly focus on computing similarity between nouns. The aim of this paper is to define a framework to compute semantic similarity even for other grammar categories such as verbs, adverbs and adjectives. The framework has been implemented on top of WordNet. Extensive experiments confirmed the suitability of this approach in the task of solving English tests.",
        "title": "Multiple Parts-of Speech",
        "doc_id": "0f023b9962e88cf9357d711af3545370",
        "_score": 130.46031,
        "objective_sentences": "We present Multiple Parts-of Speech. Abstract. The aim of this paper is to define a framework to compute semantic similarity even for other grammar categories such as verbs, adverbs and adjectives.",
        "sem_score": 1.5
    },
    {
        "date": "2015-01-01",
        "raw_text": "We present dissectHMMER: a HMMER-based score dissection framework that statistically evaluates fold-critical sequence segments for domain fold similarity. Background: Annotation transfer for function and structure within the sequence homology concept essentially requires protein sequence similarity for the secondary structural blocks forming the fold of a protein. A simplistic similarity approach in the case of non-globular segments (coiled coils, low complexity regions, transmembrane regions, long loops, etc.) is not justified and a pertinent source for mistaken homologies. The latter is either due to positional sequence conservation as a result of a very simple, physically induced pattern or integral sequence properties that are critical for function. Furthermore, against the backdrop that the number of well-studied proteins continues to grow at a slow rate, it necessitates for a search methodology to dive deeper into the sequence similarity space to connect the unknown sequences to the well-studied ones, albeit more distant, for biological function postulations. Results: Based on our previous work of dissecting the hidden markov model (HMMER) based similarity score into fold-critical and the non-globular contributions to improve homology inference, we propose a framework-dissectHMMER, that identifies more fold-related domain hits from standard HMMER searches. Subsequent statistical stratification of the fold-related hits into cohorts of functionally-related domains allows for the function postulation of the query sequence. Briefly, the technical problems as to how to recognize non-globular parts in the domain model, resolve contradictory HMMER2/HMMER3 results and evaluate fold-related domain hits for homology, are addressed in this work. The framework is benchmarked against a set of SCOP-to-Pfam domain models. Despite being a sequence-to-profile method, dissectHMMER performs favorably against a profile-to-profile based method-HHsuite/HHsearch. Examples of function annotation using dissectHMMER, including the function discovery of an uncharacterized membrane protein Q9K8K1_BACHD (WP_010899149.1) as a lactose/H+ symporter, are presented. Finally, dissectHMMER webserver is made publicly available at http://dissecthmmer.bii.a-star.edu.sg. Conclusions: The proposed framework-dissectHMMER, is faithful to the original inception of the sequence homology concept while improving upon the existing HMMER search tool through the rescue of statistically evaluated false-negative yet fold-related domain hits to the query sequence. Overall, this translates into an opportunity for any novel protein sequence to be functionally characterized.",
        "title": "dissectHMMER: a HMMER-based score dissection framework that statistically evaluates fold-critical sequence segments for domain fold similarity",
        "doc_id": "9570afe791f15358c9ccb292ce34b95e",
        "_score": 127.41505,
        "objective_sentences": "We present dissectHMMER: a HMMER-based score dissection framework that statistically evaluates fold-critical sequence segments for domain fold similarity. Results: Based on our previous work of dissecting the hidden markov model (HMMER) based similarity score into fold-critical and the non-globular contributions to improve homology inference, we propose a framework-dissectHMMER, that identifies more fold-related domain hits from standard HMMER searches.",
        "sem_score": 1.5
    },
    {
        "date": "2014-02-17",
        "raw_text": "We present Applications of corpus-based semantic similarity and word . . . . In this paper, we present a method for database schema matching: the problem of identifying elements of two given schemas that correspond to each other. Schema matching is useful in e-commerce exchanges, in data integration/warehousing, and in semantic web applications. We first present two corpus-based methods: one method is for determining the semantic similarity of two target words and the other is for automatic word segmentation. Then we present a name-based element-level database schema matching method that exploits both the semantic similarity and the word segmentation methods. Our word similarity method uses pointwise mutual information (PMI) to sort lists of important neighbor words of two target words; the words which are common in both lists are selected and their PMI values are aggregated to calculate the relative similarity score. Our word segmentation method uses corpus type frequency information to choose the type with maximum length and frequency from desegmented  text. It also uses a modified forwardbackward matching technique using maximum length frequency and entropy rate if any non-matching portions of the text exist. Finally, we exploit both the semantic similarity and the word segmentation methods in our proposed name-based element-level schema matching method. This method uses a single property (i.e., element name) for",
        "title": "Applications of corpus-based semantic similarity and word . . . ",
        "doc_id": "af85626316860cd85afbaaceac459639",
        "_score": 127.02534,
        "objective_sentences": "We present Applications of corpus-based semantic similarity and word . . . . In this paper, we present a method for database schema matching: the problem of identifying elements of two given schemas that correspond to each other.",
        "sem_score": 1.5
    },
    {
        "date": "2014-01-01",
        "raw_text": "We present Sparse Semantic Hashing for Efficient Large Scale Similarity Search. Similarity search, or finding approximate nearest neighbors, is an important technique in various large scale information retrieval applications such as document retrieval. Many recent research demonstrate that hashing methods can achieve promising results for large scale similarity search due to its computational and memory efficiency. However, most existing hashing methods ignore the hidden semantic structure of documents but only use the keyword features (e.g., tf-idf) in hashing codes learning. This paper proposes a novel sparse semantic hashing (SpSH) approach that explores the hidden semantic representation of documents in learning their corresponding hashing codes. In particular, a unified framework is designed for ensuring the hidden semantic structure among the documents by a sparse coding model, while at the same time preserving the document similarity via graph Laplacian. An iterative coordinate descent procedure is then proposed for solving the optimization problem. Extensive experiments on two large scale datasets demonstrate the superior performance of the proposed research over several state-of-the-art hashing methods.",
        "title": "Sparse Semantic Hashing for Efficient Large Scale Similarity Search",
        "doc_id": "fcbb533f46a6f83aaeaa77a455e47acc",
        "_score": 123.15106,
        "objective_sentences": "We present Sparse Semantic Hashing for Efficient Large Scale Similarity Search. This paper proposes a novel sparse semantic hashing (SpSH) approach that explores the hidden semantic representation of documents in learning their corresponding hashing codes.",
        "sem_score": 1.5
    },
    {
        "date": "2015-01-01",
        "raw_text": "We present Combining Word Representations for Measuring Word Relatedness and Similarity.. Many unsupervised methods, such as Latent Semantic Analysis and Latent Dirichlet Allocation, have been proposed to automatically infer word representations in the form of a vector. By representing a word  by  a vector, one can exploit the power of vector algebra to solve many Natural Language Processing tasks e.g. by computing the cosine similarity between the corresponding word vectors the semantic similarity between the two words can be captured. In this paper, we hypothesize that combining different word representations  complements the coverage of semantic aspects of a word and thus better represents the word than the individual representations. To this end, we present two approaches of combining word representations obtained from many heterogeneous sources. We also report empirical results for word-to-word semantic similarity and relatedness by using the new representation using two existing benchmark datasets.",
        "title": "Combining Word Representations for Measuring Word Relatedness and Similarity.",
        "doc_id": "5b6206d2932070ab806d534a4485a5c2",
        "_score": 131.81418,
        "objective_sentences": "We present Combining Word Representations for Measuring Word Relatedness and Similarity.. In this paper, we hypothesize that combining different word representations  complements the coverage of semantic aspects of a word and thus better represents the word than the individual representations.",
        "sem_score": 1.4107142857142856
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present FUSE: Multiple Network Alignment via Data Fusion. Motivation: Discovering patterns in networks of protein-protein interactions (PPIs) is a central problem in systems biology. Alignments between these networks aid functional understanding as they uncover important information, such as evolutionary conserved pathways, protein complexes and functional orthologs. However, the complexity of the multiple network alignment problem grows exponentially with the number of networks being aligned, and designing a multiple network aligner that is both scalable and that produces biologically relevant alignments is a challenging task that has not been fully addressed. The objective of multiple network alignment is to create clusters of nodes that are evolutionarily and functionally conserved across all networks. Unfortunately, the alignment methods proposed thus far do not meet this objective as they are guided by pairwise scores that do not utilize the entire functional and evolutionary information across all networks. Results: To overcome this weakness, we propose Fuse, a new multiple network alignment algorithm that works in two steps. First, it computes our novel protein functional similarity scores by fusing information from wiring patterns of all aligned PPI networks and sequence similarities between their proteins. This is in contrast with the previous tools that are all based on protein similarities in pairs of networks being aligned. Our comprehensive new protein similarity scores are computed by Non-negative Matrix TriFactorization (NMTF) method that predicts associations between proteins whose homology (from sequences) and functioning similarity (from wiring patterns) are supported by all networks. Using the five largest and most complete PPI networks from BioGRID, we show that NMTF predicts a large number protein pairs that are biologically consistent. Second, to identify clusters of aligned proteins over all networks, Fuse uses our novel maximum weight k-partite matching approximation algorithm. We compare Fuse with the state of the art multiple network aligners and show that: (1) by using only sequence alignment scores, Fuse already outperforms other aligners and produces a larger number of biologically consistent clusters that cover all aligned PPI networks, and (2) using both sequence alignments and topological NMTF-predicted scores leads to the best multiple network alignments thus far. Availability: Our dataset and software are freely available from the web site:",
        "title": "FUSE: Multiple Network Alignment via Data Fusion",
        "doc_id": "6c31cc47e377e06f836d9a35fa08a003",
        "_score": 113.9939,
        "objective_sentences": "We present FUSE: Multiple Network Alignment via Data Fusion. The objective of multiple network alignment is to create clusters of nodes that are evolutionarily and functionally conserved across all networks.",
        "sem_score": 1.4
    },
    {
        "date": "2016-12-01",
        "raw_text": "We present Fine-Tuning an Algorithm for Semantic Document Clustering Using a Similarity Graph. In this article, we examine an algorithm for document clustering using a similarity graph. The graph stores words and common phrases from the English language as nodes and it can be used to compute the degree of semantic similarity between any two phrases. One application of the similarity graph is semantic document clustering, that is, grouping documents based on the meaning of the words in them. Since our algorithm for semantic document clustering relies on multiple parameters, we examine how fine-tuning these values affects the quality of the result. Specifically, we use the Reuters-21578 benchmark, which contains [Formula: see text] newswire stories that are grouped in 82 categories using human judgment. We apply the k-means clustering algorithm to group the documents using a similarity metric that is based on keywords matching and one that uses the similarity graph. We evaluate the results of the clustering algorithms using multiple metrics, such as precision, recall, f-score, entropy, and purity.",
        "title": "Fine-Tuning an Algorithm for Semantic Document Clustering Using a Similarity Graph",
        "doc_id": "d10616e0784ea3b80f0e046bbb65855e",
        "_score": 121.84516,
        "objective_sentences": "We present Fine-Tuning an Algorithm for Semantic Document Clustering Using a Similarity Graph. In this article, we examine an algorithm for document clustering using a similarity graph.",
        "sem_score": 1.3636363636363638
    },
    {
        "date": "2014-01-01",
        "raw_text": "We present Multi-level sequence alignment: a trade-off between speed and accuracy in similar text searching. A fingerprinting algorithm and sequence alignment are used widely to calculate the similarity of documents. The fingerprinting method is simple and fast but it cannot find specific similar regions. A string alignment method is used to identify similar regions by arranging sequences of strings. This has the advantage that it can find specific similar regions, but it also has the disadvantage that it requires more computational time. Multi-level alignment (MLA) is a new method, which was designed to exploit the advantages of both methods. MLA divides input documents into uniform length blocks, before extracting the fingerprint from each block and calculating the similarity of block pairs by comparing fingerprints. A similarity table is created during this process. Finally, sequence alignment is used to identify the longest similar regions in the similarity table. MLA allows users to change the block's size to control the relative proportion of the fingerprint algorithm and sequence alignment. A document is divided into several block, so similar regions are also fragmented into two or more blocks. To address this fragmentation problem, we propose a united block method. The united block method integrates adjacent fragmented similar regions to increase the similarity value. Our experiments demonstrated that computing a document's similarity using the united block method was more accurate than the original MLA method, with minor reductions in time.",
        "title": "Multi-level sequence alignment: a trade-off between speed and accuracy in similar text searching",
        "doc_id": "0ab463befa81c9537194fdf354d3bc49",
        "_score": 155.78374,
        "objective_sentences": "We present Multi-level sequence alignment: a trade-off between speed and accuracy in similar text searching. To address this fragmentation problem, we propose a united block method.",
        "sem_score": 1.3529411764705883
    },
    {
        "date": "2014-01-01",
        "raw_text": "We present Combination Features for Semantic Similarity Measure. Computing the semantic similarity between words is one of the key tasks in many language-based applications. Recent work has focused on using contextual clues for semantic similarity computation. In this paper, we propose a method to the measure semantic similarity between words using plain text contents. It takes into account information attributes (local) and topic information (global) of words to disclose their semantic similarity scores. The method models the representation of a word as a high dimensional vector of word attributes and latent topics. Thus, the semantic similarity between two words is measured by the semantic distance between their respective vectors. We have tested the proposed method on WordSimilarity-353 dataset. The empirical results have shown the combination features contribute to improve the semantic similarity results the dataset in comparison with previous work on the same task using plain text contents.",
        "title": "Combination Features for Semantic Similarity Measure",
        "doc_id": "172904dbb9bf1e1fa2ebb710a32970c9",
        "_score": 153.09381,
        "objective_sentences": "We present Combination Features for Semantic Similarity Measure. In this paper, we propose a method to the measure semantic similarity between words using plain text contents.",
        "sem_score": 1.3333333333333335
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present Document representation based on semantic smoothed topic model. The goal of document representation is to capture certain feature of the document. Many existing document representation methods are based on bag-of-words and ignore semantic relevance between words in the document. There we proposed a semantic smoothed topic model to represent document. It takes semantic similarity into consideration for topic of document. We conducted two experiments utilizing this method for text classification and information retrieval task. The experimental results suggest that our method is useful for capturing the semantic of text to alleviating polysemy and synonyms problem and data sparseness problem.",
        "title": "Document representation based on semantic smoothed topic model",
        "doc_id": "013d88571ca6093b44f5b868c3311c47",
        "_score": 149.01476,
        "objective_sentences": "We present Document representation based on semantic smoothed topic model. The goal of document representation is to capture certain feature of the document.",
        "sem_score": 1.3333333333333333
    },
    {
        "raw_text": "We present Accurate multiple alignment of distantly related genome sequences using filtered spaced word matches as anchor points. Abstract Motivation Most methods for pairwise and multiple genome alignment use fast local homology search tools to identify anchor points , i.e. high-scoring local alignments of the input sequences. Sequence segments between those anchor points are then aligned with slower, more sensitive methods. Finding suitable anchor points is therefore crucial for genome sequence comparison; speed and sensitivity of genome alignment depend on the underlying anchoring methods. Results In this article, we use filtered spaced word matches to generate anchor points for genome alignment. For a given binary pattern representing match and don\u2019t-care positions, we first search for spaced-word matches , i.e. ungapped local pairwise alignments with matching nucleotides at the match positions of the pattern and possible mismatches at the don\u2019t-care positions. Those spaced-word matches that have similarity scores above some threshold value are then extended using a standard X -drop algorithm; the resulting local alignments are used as anchor points. To evaluate this approach, we used the popular multiple-genome-alignment pipeline Mugsy and replaced the exact word matches that Mugsy uses as anchor points with our spaced-word-based anchor points. For closely related genome sequences, the two anchoring procedures lead to multiple alignments of similar quality. For distantly related genomes, however, alignments calculated with our filtered-spaced-word matches are superior to alignments produced with the original Mugsy program where exact word matches are used to find anchor points. Availability and implementation  http://spacedanchor.gobics.de  Supplementary information  Supplementary data are available at Bioinformatics online.",
        "date": "2019-01-15",
        "title": "Accurate multiple alignment of distantly related genome sequences using filtered spaced word matches as anchor points",
        "doc_id": "36d2b6e24b4c9a0fbfadbaec685f6e42",
        "_score": 135.43286,
        "objective_sentences": "We present Accurate multiple alignment of distantly related genome sequences using filtered spaced word matches as anchor points. In this article, we use filtered spaced word matches to generate anchor points for genome alignment.",
        "sem_score": 1.3333333333333333
    },
    {
        "date": "2018-02-18",
        "raw_text": "We present Calculating the similarity between words and sentences using a lexical database and corpus statistics. Calculating the semantic similarity between sentences is a long dealt problem in the area of natural language processing. The semantic analysis field has a crucial role to play in the research related to the text analytics. The semantic similarity differs as the domain of operation differs. In this paper, we present a methodology which deals with this issue by incorporating semantic similarity and corpus statistics. To calculate the semantic similarity between words and sentences, the proposed method follows an edge-based approach using a lexical database. The methodology can be applied in a variety of domains. The methodology has been tested on both benchmark standards and mean human similarity dataset. When tested on these two datasets, it gives highest correlation value for both word and sentence similarity outperforming other similar models. For word similarity, we obtained Pearson correlation coefficient of 0.8753 and for sentence similarity, the correlation obtained is 0.8794. ",
        "title": "Calculating the similarity between words and sentences using a lexical database and corpus statistics",
        "doc_id": "1ffff7354173311d93a75718ba0dafd4",
        "_score": 121.0883,
        "objective_sentences": "We present Calculating the similarity between words and sentences using a lexical database and corpus statistics.",
        "sem_score": 1.3333333333333333
    },
    {
        "date": "2018-08-14",
        "raw_text": "We present Head to Head: Semantic Similarity of Multi\u2013Word Terms. Terms are linguistic signifiers of domain-specific concepts. Semantic similarity between terms refers to the corresponding distance in the conceptual space. In this paper, we use lexico-syntactic information to define a vector space representation in which cosine similarity closely approximates semantic similarity between the corresponding terms. Given a multi-word term, each word is weighed in terms of its defining properties. In this context, the head noun is given the highest weight. Other words are weighed depending on their relations to the head noun. We formalized the problem as that of determining a topological ordering of a direct acyclic graph, which is based on constituency and dependency relations within a noun phrase. To counteract the errors associated with automatically inferred constituency and dependency relations, we implemented a heuristic approach to approximating the topological ordering. Different weights are assigned to different words based on their positions. Clustering experiments performed on such a vector space representation showed considerable improvement over the conventional bag-of-word representation. Specifically, it more consistently reflected semantic similarity between the terms. This was established by analyzing the differences between automatically generated dendrograms and manually constructed taxonomies. In conclusion, our method can be used to semi-automate taxonomy construction.",
        "title": "Head to Head: Semantic Similarity of Multi\u2013Word Terms",
        "doc_id": "7e37aa112060ffc3fc7e7e2b434363b9",
        "_score": 114.989006,
        "objective_sentences": "We present Head to Head: Semantic Similarity of Multi\u2013Word Terms. In this paper, we use lexico-syntactic information to define a vector space representation in which cosine similarity closely approximates semantic similarity between the corresponding terms.",
        "sem_score": 1.3333333333333333
    },
    {
        "date": "2014-01-01",
        "raw_text": "We present Domain-Independent Unsupervised Text Segmentation for Data Management. In this study, we have proposed a domain-independent unsupervised text segmentation method, which is applicable to even if unseen single document. This proposed method segments text documents by evaluating similarity between sentences. It is generally difficult to calculate semantic similarity between words that comprise sentences when the domain knowledge is insufficient. This problem influences segmentation accuracy. To address this problem, we use word 2 vec to calculate semantic similarity between words. Using word 2 vec, we embed semantic relationships between words in a vector space by training with large domain-independent corpora. Furthermore, we combine semantic and collocation similarities, i.e., The features between words within a document. The proposed method applies this combined similarity to affinity propagation clustering. Similarity between sentences is defined based on the earth mover's distance between the frequencies of the obtained topical clusters. After calculating similarity between sentences, segmentation boundaries are automatically optimized using dynamic programming. The experimental results obtained using two datasets show that the proposed method clearly outperforms state-of-the-art domain-independent approaches and obtains equal performance with state-of-the-art domain-dependent approaches such as those that use topic modeling.",
        "title": "Domain-Independent Unsupervised Text Segmentation for Data Management",
        "doc_id": "38f51e4ee188220c4abe5df20da9c686",
        "_score": 149.8707,
        "objective_sentences": "We present Domain-Independent Unsupervised Text Segmentation for Data Management. In this study, we have proposed a domain-independent unsupervised text segmentation method, which is applicable to even if unseen single document.",
        "sem_score": 1.3333333333333333
    },
    {
        "date": "2016-01-01",
        "raw_text": "We present Document representation based on semantic smoothed topic model. The goal of document representation is to capture certain feature of the document. Many existing document representation methods are based on bag-of-words and ignore semantic relevance between words in the document. There we proposed a semantic smoothed topic model to represent document. It takes semantic similarity into consideration for topic of document. We conducted two experiments utilizing this method for text classification and information retrieval task. The experimental results suggest that our method is useful for capturing the semantic of text to alleviating polysemy and synonyms problem and data sparseness problem.",
        "title": "Document representation based on semantic smoothed topic model",
        "doc_id": "013d88571ca6093b44f5b868c3311c47",
        "_score": 149.0288,
        "objective_sentences": "We present Document representation based on semantic smoothed topic model. The goal of document representation is to capture certain feature of the document.",
        "sem_score": 1.3333333333333333
    },
    {
        "date": "2015-01-01",
        "raw_text": "We present Semantic Similarity Measures Between Words within a Document using WordNet. Semantic similarity between words can be applied in many fields including computational linguistics, artificial intelligence, and information retrieval. In this paper, we present weighted method for measuring a semantic similarity between words in a document. This method uses edge distance and depth of WordNet. The method calculates a semantic similarity between words on the basis of document information. Document information uses word term frequencies(TF) and word concept frequencies(CF). Each word weight value is calculated by TF and CF in the document. The method includes the edge distance between words, the depth of subsumer, and the word weight in the document. We compared out scheme with the other method by experiments. As the result, the proposed method outperforms other similarity measures. In the document, the word weight value is calculated by the proposed method. Other methods which based simple shortest distance or depth had difficult to represent the information or merge informations. This paper considered shortest distance, depth and information of words in the document, and also improved the performance.",
        "title": "Semantic Similarity Measures Between Words within a Document using WordNet",
        "doc_id": "5f6bdd11556d47c929c3f6d692418328",
        "_score": 149.07771,
        "objective_sentences": "We present Semantic Similarity Measures Between Words within a Document using WordNet. In this paper, we present weighted method for measuring a semantic similarity between words in a document.",
        "sem_score": 1.3157894736842106
    }
]